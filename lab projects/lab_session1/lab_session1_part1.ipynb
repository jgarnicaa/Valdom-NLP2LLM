{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All Rights Reserved**\n",
    "\n",
    "**Copyright (c) 2025 IRT Saint-Exupery**\n",
    "\n",
    "*Author & contact:* \n",
    "* mouhcine.mendil@irt-saintexupery.com \n",
    "\n",
    "# Natural Language Processing (NLP) to Large Language Models (LLM)\n",
    "\n",
    "<div align=\"center\">\n",
    "    <h2>Lab Session 1: Part I</h2>\n",
    "</div>\n",
    "\n",
    "By now, you may have encountered various data structures such as tables, images, and time series. But have you ever wondered how computers can understand human language? This is where Natural Language Processing (NLP) comes in: it is one of the most popular fields in AI today. Thanks to recent developments in foundation models that rely on transformer architectures, NLP capabilities have has seen revolutionary advancements. In this tutorial, you will get an introduction to this exciting field that combines computer science, linguistics, and machine learning. We will explore the world of text data and provide you with the skills necessary to process them and create models that can unlock knowledge and extract valuable insights.\n",
    "\n",
    "## 0. Setup\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "There are many special characters for text formatting that control rendering like line jumps, symbols, etc. \n",
    "\n",
    "⚠️ Use <code>print</code> for raw text output and the function <code>printmd</code> (defined below) to have a beautiful markdown rendering when needed.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP tools requirements:**\n",
    "\n",
    "* Natural Language Toolkit (NLTK) is one of the leading platforms for building Python programs to work with human language data.\n",
    "\n",
    "* Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is NLP and information retrieval (IR) community.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install requirements\n",
    "!pip install --upgrade nltk pandas gensim seaborn scikit-learn textblob regex tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sentiment Analysis of IMDb Movie Reviews\n",
    "\n",
    "One of the most common applications of NLP is text classification. Therefore, you will kickoff this notebook by addressing the task of **sentiment analysis**. The aim is understanding the positive or negative sentiment/opinion expressed in text. \n",
    "\n",
    "## 1.1 IMDb Dataset \n",
    "\n",
    "IMDb dataset contains 50,000 movie reviews in English, each labelled as positive or negative. You'll find it under `data` folder (downloaded from [Kaggle](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)).\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "\n",
    "<b> Exercise 1.1 </b>\n",
    "\n",
    "- Load the dataset in a dataframe. Note the feature column (review) and target column (sentiment).\n",
    "- How many samples are there ?\n",
    "- Plot the target distribution. Is the dataset balanced ?\n",
    "- Transform the target values to binary (i.e \"positive\" -> 1 & \"negative\" -> 0). Use [`np.where`](https://numpy.org/doc/stable/reference/generated/numpy.where.html).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: load csv file\n",
    "df_raw = ...\n",
    "\n",
    "## TODO: count the number of samples\n",
    "...\n",
    "\n",
    "## TODO: plot the distribution of sentiments\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: binarize the \"sentiment column\"\n",
    "\n",
    "df_raw[\"sentiment\"] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(df_raw[\"sentiment\"]) == set(\n",
    "    {0, 1}\n",
    "), \"You still have non-binarized values, try again\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data cleaning\n",
    "\n",
    "Textual data require special handling in the pre-processing phase. \n",
    "\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "\n",
    "<b> Exercise 1.2.1.1 </b>\n",
    "\n",
    "- Take a look on the first two reviews using <code>print</code> and <code>printmd</code>. What do you notice ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the first two reviews using print and printmd\n",
    "\n",
    "# First review\n",
    "printmd(\"### First review\")\n",
    "...  # print\n",
    "...  # printmd\n",
    "\n",
    "# Second review\n",
    "printmd(\"-------\")\n",
    "printmd(\"### Second review\")\n",
    "...  # print\n",
    "...  # printmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h4><b>Intermediary Exercises</b></h4>\n",
    "\n",
    "Strings in Python are sequences of characters, which allows us to manipulate and process them in various ways. Let's explore:\n",
    "\n",
    "<ol>\n",
    "  <li>\n",
    "    <b>Extract characters:</b> \n",
    "    Print the list of <strong>characters</strong> that form the first review.\n",
    "  </li>\n",
    "  <li>\n",
    "    <b>Extract words:</b> \n",
    "    If we want to extract words instead of characters, we can use the \n",
    "    <a href=\"https://docs.python.org/3/library/stdtypes.html#str.split\" target=\"_blank\"><code>split</code></a> \n",
    "    method. By specifying a suitable separator (e.g., a space for English and French), we can break the string into words.\n",
    "    <ul>\n",
    "      <li>Print the list of <strong>words</strong> forming the first review.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <b>Rebuild the original Review:</b> \n",
    "    Concatenate the extracted list of words back into the original sentence using the \n",
    "    <a href=\"https://docs.python.org/3/library/stdtypes.html#str.join\" target=\"_blank\"><code>join</code></a> \n",
    "    method. Use a space to interleave each word.\n",
    "  </li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Print the characters of the first review\n",
    "char_list = []\n",
    "...  # extract characters from the first review\n",
    "printmd(\"### Characters of the first review\")\n",
    "print(char_list)\n",
    "\n",
    "# 2. Print the words of the first review\n",
    "first_review_words = ...  # extract words from the first review\n",
    "printmd(\"### Words of the first review\")\n",
    "print(first_review_words)\n",
    "\n",
    "# 3. Rebuild the first review from the list of words\n",
    "first_review_rebuilt = ...\n",
    "printmd(\"### Rebuilt first review\")\n",
    "print(first_review_rebuilt)\n",
    "\n",
    "assert (\n",
    "    first_review_rebuilt == df_raw[\"review\"][0]\n",
    "), \"The review is not correctly rebuilt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "<b> Exercise 1.2.1.2 </b>\n",
    "\n",
    "- Text data usually contains artifacts only relevent for visualization (e.g., HTML tags and special characters) and undesired content such as spelling mistakes. Such elements are not only useless for modeling but can also be harmful as they pollute the relevant information. For each of the following data cleaning operations, write a function to apply them on the IMDB dataset:    \n",
    "    1. Lowering all capital letters.\n",
    "    2. Using the [regex](https://docs.python.org/3/library/re.html) Python library (`re.sub`), substitute (by a space) the patterns associated to Hyperlinks (\"https:something\" or \"http:something\"), Mentions (\"@something\") and HTML elements (\"\\<something>\" or \"\\</something>\").\n",
    "    3. Stop words, i.e commonly occurring words in a language that are considered to have minimal meaning on their own.\n",
    "        * Using <code>stopwords.words</code>, list all stopwords in english. \n",
    "        * Delete stopwords from IMDB movie reviews\n",
    "    4. **Replace with space any special characters**, that is anything other than alpha-numerical characters, such as punctuation and symbols (you can use [isalnum](https://docs.python.org/3/library/stdtypes.html#str.isalnum)).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(text):\n",
    "    # Lowercase text\n",
    "    lowered_text = ...\n",
    "    return lowered_text\n",
    "\n",
    "\n",
    "def remove_patterns(text):\n",
    "    # replace hyperlinks, mentions and html elements\n",
    "    patterns = \"https?:\\\\S+|http?:\\\\S+|@\\\\S+|<[^<]+?>\"\n",
    "    replacement_text = \" \"\n",
    "    cleaned_text = ...\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "english_stop_words = ...  # TODO\n",
    "printmd(\"### English stop words\")\n",
    "print(f\"There are {len(english_stop_words)} stop words in english: \\n\")\n",
    "print(english_stop_words)\n",
    "\n",
    "\n",
    "def clean_stop_words(text):\n",
    "    english_stop_words = ...\n",
    "    # list of the words we will keep\n",
    "    kept_words_list = []\n",
    "    # Parse text and keep non-stop words\n",
    "    ...\n",
    "    # return the text without the stop words\n",
    "    return ...\n",
    "\n",
    "\n",
    "def clean_non_alphanum(text):\n",
    "    # Remove non alphanumeric characters\n",
    "    clean_characters_list = []\n",
    "    # Loop over the characters of the text to keep only the alphanumeric ones\n",
    "    # and replace others by a space\n",
    "    ...\n",
    "    # join the characters to rebuild the text\n",
    "    cleaned_text = ...\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "<b> Exercise 1.2.1.3 </b>\n",
    "- Apply the previous operations on a copy of the raw dataframe and compare the first review before and after cleaning. Use [`apply`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) to sequentially apply the preprocessing functions (⚠️ **order is important**). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init cleaned dataframe, we keep the copy of raw dataframe untouched\n",
    "df_clean = deepcopy(df_raw)\n",
    "\n",
    "# Apply operations\n",
    "list_operations = ...\n",
    "for op in tqdm(list_operations):\n",
    "    df_clean.review = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the first review before and after cleaning\n",
    "printmd(\"### First review before cleaning\")\n",
    "printmd(df_raw[\"review\"][0])\n",
    "printmd(\"-----\")\n",
    "printmd(\"### First review after cleaning\")\n",
    "printmd(df_clean[\"review\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**\n",
    "\n",
    "<blockquote>\n",
    "In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root\n",
    "</blockquote>\n",
    "\n",
    "[-- Wikipedia](https://en.wikipedia.org/wiki/Stemming)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"figures/stemming.png\" width=\"50%\"/>\n",
    "  <figcaption>Source: Quora</figcaption>\n",
    "</div>\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "\n",
    "<b> Exercise 1.2.2 </b>\n",
    "\n",
    "- NLTK offers several stemmers. Write a function `apply_stemmer` to apply the `Lancaster` stemmer (known for its accuracy for English text).\n",
    "- Apply the stemmer on a copy of the cleaned dataset. Compare the sentences before and after stemming; what do you notice ?\n",
    "\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "\n",
    "def apply_stemmer(text):\n",
    "    # Split text into words\n",
    "    words = ...\n",
    "    # Apply stemming to each word\n",
    "    stemmed_words = ...\n",
    "    # Join the stemmed words back into a string\n",
    "    stemmed_text = ...\n",
    "    return stemmed_text\n",
    "\n",
    "\n",
    "# Init stem dataframe, we keep the copy the cleaned dataframe untouched\n",
    "df_stem = deepcopy(df_clean)\n",
    "# Apply stemming\n",
    "df_stem.review = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the cleaned first review before and after stemming\n",
    "printmd(\"### Cleaned first review before stemming\")\n",
    "printmd(df_clean[\"review\"][0])\n",
    "printmd(\"-----\")\n",
    "printmd(\"### Cleaned first review after stemming\")\n",
    "printmd(df_stem[\"review\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ By grouping together words with similar meanings into their base form, stemming reduces the overall number of unique words the model needs to deal with. Stemming helps to normalize text data by handling variations of words due to tense, plurals, or derivational suffixes, which can be beneficial for tasks where a smaller vocabulary can improve efficiency. However, stemming can sometimes lead to the creation of non-words or words with altered meanings. \n",
    "\n",
    "Lemmatization is another option that is more accurate and preserves meaning better, but can be computationally more expensive.\n",
    "\n",
    "IMDb dataset is simple enough, **so we will not use stemming or lemmatization**. But note that these two operations exist and can be helpful for more complicated dataset or tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data Preparation\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "\n",
    "<b> Exercise 1.3.1 </b>\n",
    "\n",
    "- Split the **clean data** into train (80%) and test (20%) subsets. Use [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with a random seed $=0$.\n",
    "- Ensure the train and test subsets are balanced by visualizing the distributions of sentiments in both subsets. Use a plot to confirm that the sentiment distribution is similar in the two subsets.\n",
    "\n",
    "<p>\n",
    "<b>Note:</b> Typically, datasets are split into three subsets: <strong>train</strong>, <strong>validation</strong>, and <strong>test</strong>. Since we are not performing hyperparameter tuning nor regularization in this exercise, we will omit the validation set. However, when tuning models, always reserve a portion of your data specifically for validation.\n",
    "</p>\n",
    "\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split from sklearn\n",
    "...\n",
    "\n",
    "df_train, df_test = ...\n",
    "\n",
    "# Check the sentiment distribution of the train and test sets\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "# Plot the sentiment distribution of the train set on the first subplot\n",
    "...\n",
    "# Plot the sentiment distribution of the test set on the second subplot\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**\n",
    "\n",
    "To build a model for our task, we need to further preprocess the text by choping it into words or subwords called **tokens**, instead of individual characters.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"figures/tokenization.png\" width=\"40%\"/>\n",
    "  <figcaption>Author: Shann Khosla<figcaption/>\n",
    "</div>\n",
    "\n",
    "For our task on IMDb dataset, we will use spaces for token boundaries.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "⚠️⚠️⚠️ It's important to note that using spaces to separate words may not be appropriate for all languages. For instance, Chinese writing doesn't use spaces between words, Vietnamese uses spaces even within words, and German often combines multiple words without spaces. Even in English, spaces are not always the best way to tokenize text, as seen in examples like \"hot dog\" or \"#funnyvideos.\" ⚠️⚠️⚠️\n",
    "\n",
    "To address these issues, there are several methods to tokenize and detokenize text at the subword level. We can cite for example Byte Pair Encoding (BPE), Unigram language modeling (ULM), WordPiece and SentencePiece. You can find many state-of-the-art, fast and optimized tokenizes in [the tokenizers library by Hugging Face](https://huggingface.co/docs/tokenizers/index).\n",
    "</div>\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "\n",
    "<b> Exercise 1.3.2 </b>\n",
    "- Perform a <strong>word-level tokenization</strong> using <code>nltk.tokenize.word_tokenize</code>.\n",
    "- Analyze the behavior of <code>word_tokenize</code>. Does it simply split text based on spaces, or does it handle text in a more nuanced way? Provide an explanation based on your observations.    \n",
    "\n",
    "<div/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "\n",
    "# tokenization function\n",
    "def tokenize_text(text):\n",
    "    tokens = ...\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Init stem dataframe, we keep the copy the cleaned dataframe untouched\n",
    "df_train_tokenized = deepcopy(df_train)\n",
    "df_test_tokenized = deepcopy(df_test)\n",
    "\n",
    "# Apply tokenization\n",
    "df_train_tokenized.review = ...\n",
    "df_test_tokenized.review = ...\n",
    "\n",
    "df_train_tokenized.reset_index(inplace=True, drop=True)\n",
    "df_test_tokenized.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick overview of some important NLP terminology:\n",
    "\n",
    "- **Document**: a single sample text in the dataset (e.g., a single movie review).\n",
    "\n",
    "- **Corpus**:  a large collection of documents (reviews) in the dataset.\n",
    "\n",
    "- **Vocabulary**:  The complete set of unique tokens for the training corpus. The vocabulary is a subset of the words used for training that might exist in the broader corpus, and should therefore be comprehensive to avoid encountering a word not in its vocabulary (i.e.,  Out-of-Vocabulary words). \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "⚠️ Usually, limitation are applied to the vocabulary size which typically involves selecting the most frequent tokens, since it’s unlikely that very rare words will be important for the task. Limiting the vocabulary size will reduce the number of parameters the model needs to learn. \n",
    "\n",
    "In real-world scenarios, we cannot precisely predict the tokens we will encounter during operation. Therefore, to avoid bias, you should construct the vocabulary (and corpus) based solely on your **training set**.\n",
    "</div>\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 1.3.3 </b>\n",
    "\n",
    "* Recover all the tokens from the training corpus and store them in a list. We recommend using [`pandas.DataFrame.explode`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html).\n",
    "* Count the number of unique tokens that constitute the full vocabulary.\n",
    "* Reduce the vocabulary to the 10,000 most frequent tokens.\n",
    "* Store the result in a dictionary `vocab_token2index`, where each token (key) is assigned a unique integer ID (for example, a value between 0 and 9999).\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all tokens from df_train_tokenized\n",
    "training_tokens = ...\n",
    "\n",
    "# Count the number of tokens in the vocabulary\n",
    "print(f\"The number of tokens in the full vocabulary is: {...}\")\n",
    "\n",
    "# Reduce size to the 10000th most frequent\n",
    "vocabulary = ...\n",
    "\n",
    "# token to index dict\n",
    "vocab_token2index = ...\n",
    "\n",
    "# Count the number of tokens in the limited vocabulary\n",
    "print(f\"The number of tokens in the filtered vocabulary is: {len(vocab_token2index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization\n",
    "\n",
    "Text Vectorization consists of converting textual data into numerical representations. This makes it possible to process textual data in machine learning algorithms that typically work with numerical values. Common vectorization techniques include:\n",
    "\n",
    "* **One-Hot Encoding (OHE)**: represents a token as a binary vector that has the size of the vocabulary. Each token is assigned a unique position in the vector, corresponding to its index in the vocabulary. In the relevant position, the vector has $1$ to indicate the token's presence and $0$ otherwise. This approach is simple but turns out to be inefficient for large vocabularies. For instance, if there are 50,000 tokens in the vocabulary, one-hot encoding would create a 50,000-dimensional **sparse** vector (which mostly contains zeros) per token. If we need a encoding at the document level, a strategy needs to be adopted to aggregate the one-hot representations of the tokens composing the documents.  \n",
    "\n",
    "* **Bag-of-Words (BoW)**: represents text data as a numerical vector, where each unique word in the vocabulary corresponds to a dimension in the vector space. Unlike one-hot encoding, which uses binary values to indicate the presence of a single token, BoW counts the occurrences of each token in a document. This representation summarizes the document's content while ignoring token order, context, and semantics.\n",
    "\n",
    "* **Term Frequency- Inverse Document Frequency (TF-IDF)**:is a numerical statistic used to evaluate the importance of a token (word or term) in a document relative to a collection of documents (corpus). It scores a token by multiplying its Term Frequency (TF) by its Inverse Document Frequency (IDF):\n",
    "    * Term Frequency (TF): The frequency of a token $t$ in a document $d$, normalized by the total number of tokens in the document $d$:\n",
    "    $$TF(t, d)= \\frac{\\text{Number of times the token $t$ appears in the document $d$}}{\\text{Total number of tokens in document $d$}}$$\n",
    "    * Inverse Document Frequency (IDF): A measure of how unique or rare a token $t$ is across the corpus. Tokens that appear in fewer documents (e.g., technical terms) are assigned higher importance than those common in all documents (e.g., stop words like \"a,\" \"the\").\n",
    "    $$IDF(t)= \\log(\\frac{\\text{Number of documents in the corpus}}{\\text{Number of documents in the corpus containing the token $t$}})$$\n",
    "    * The TF-IDF The TF-IDF score of a token $t$ in a document $d$ is the product of its TF and IDF values:\n",
    "    $$TF\\text{-}IDF(t, d) = TF(t, d) \\cdot IDF(t)$$\n",
    "\n",
    "* **Embeddings**: An embedding is a numerical representation of complex data, where each token (e.g., word or phrase) is represented as a vector in an $n$-dimensional space. Unlike traditional vectorization techniques like Bag-of-Words or TF-IDF, embeddings are typically compact, dense, and highly informative representations. One of the main advantages of embeddings is their ability to encode semantic relationships. This means that words with similar meanings (synonyms) are represented by vectors that are closer together in the $n$-dimensional space. There are many technique to efficiently learn embeddings in an unsupervised way, some of the most popular **word embedding models** are Word2vec, GloVe, fastText, ELMo and BERT. **You can find online already pretrained models for these popular embeddings**.  \n",
    "<div align=\"center\">\n",
    "<img src=\"figures/taxonomy.png\"/>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Encoders\n",
    "\n",
    "Let's first explore traditional encoding techniques on our data, including BoW and TF-IDF. Keep in mind that we can flexibly choose the encoding technique that goes with our classification models (remember, our goal is to perform sentiment analysis on IMDb reviews).  \n",
    "\n",
    "In the next section, we will spend some time on embedding to understand and visualize how they process the data.\n",
    "\n",
    "</div>\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 1.3.4 </b>\n",
    "\n",
    "* Implement the **Bag-of-Words (BoW) vectorizer** using the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from scikit-learn.\n",
    "* Apply the BoW vectorization on the training data.\n",
    "* Verify that the first reviews in the training set have been correctly vectorized. How can you confirm this?  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_encoder = CountVectorizer(\n",
    "    vocabulary=..., tokenizer=lambda x: x, preprocessor=lambda x: x\n",
    ")\n",
    "X_train_bow = ...\n",
    "y_train = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the vectorized data\n",
    "# --------------------------\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 1.3.5 </b>\n",
    "\n",
    "* Implement the **TF-IDF vectorizer** using the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from scikit-learn.\n",
    "* Apply the TF-IDF vectorization on the training data.\n",
    "* Pick some reviews and print their 10 most important tokens based on their TF-IDF scores.  \n",
    "* Reflect: Can you infer whether the associated sentiment is positive or negative based on the most important tokens?\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_encoder = TfidfVectorizer(...)\n",
    "X_train_tfidf = ...\n",
    "y_train = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4  Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn how to use word embedding models and explore some interesting features on specific examples.\n",
    "\n",
    "</div>\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 1.4.1 </b>\n",
    "\n",
    "\n",
    "* Using gensim [downloader](https://radimrehurek.com/gensim/downloader.html), load a pretrained word embedding model. You can find the list of available models [here](https://github.com/piskvorky/gensim-data?tab=readme-ov-file#models) (suggestion: start with `glove-wiki-gigaword-100`).\n",
    "\n",
    "* Use the word embedding model to vectorize the following tokens: `[\"man\", \"woman\", \"king\", \"queen\", \"prince\", \"princess\", \"actor\", \"actress\", \"movies\"]`\n",
    "\n",
    "* What is the dimension of the embedding space ? \n",
    "\n",
    "* Try loading another model and compare the size of the embedding vectors.\n",
    "\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"man\",\n",
    "    \"woman\",\n",
    "    \"king\",\n",
    "    \"queen\",\n",
    "    \"prince\",\n",
    "    \"princess\",\n",
    "    \"actor\",\n",
    "    \"actress\",\n",
    "    \"movies\",\n",
    "]\n",
    "\n",
    "# Get the vectors for the words\n",
    "word_embeddings = ...\n",
    "printmd(f\"The dimension of the embeddings is: {...}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings often have hundreds (or even thousands) of dimensions that capture the meaning and relationships of words. However, such high-dimensional objects are difficult to visualize directly.\n",
    "\n",
    "There is a way to visualize these vectors using machine learning techniques for dimensionality reduction such as Principle Component Analysis (PCA). We will project the high-dimensional word embeddings into a 2D space, while preserving the relative distances between similar words. This helps visualize semantic relationships embedded in the original high-dimensional space. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "⚠️ BoW and TF-IDF vectorizations can also be visualized using dimensionality reduction. But since they don't capture semantic relationships, the distance between words doesn't necessarily represent their actual similarities in meaning. ⚠️\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 1.4.2 </b>\n",
    "\n",
    "* Use [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to project the embeddings of the previous exercise into a 2D space.\n",
    "* Visualize the 2D word embeddings using a scatter plot.\n",
    "* Compute the vector `embedding(\"actor\")-embedding(\"man\")+embedding(\"woman\")` and add it to the same scatter plot.\n",
    "* Interpret the resulting vector `mystery_embedding` and find its most similar word using [`similar_by_vector`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.similar_by_vector)?\n",
    "* Reflect: do arithmetic operations on word embeddings preserve semantic meaning?\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Init PCA object\n",
    "pca = ...\n",
    "# fit/project embeddings in 2D\n",
    "projected_embeddings = ...\n",
    "\n",
    "# Arithmetic embedding\n",
    "mystery_embedding = ...\n",
    "mystery_embedding_projected = ...\n",
    "\n",
    "# add mystery embedding to the words\n",
    "if \"mystery\" not in words:\n",
    "    words.append(\"mystery\")\n",
    "projected_embeddings = np.vstack([projected_embeddings, mystery_embedding_projected])\n",
    "\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(...)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (projected_embeddings[i, 0], projected_embeddings[i, 1]))\n",
    "\n",
    "plt.title(\"PCA projection of word embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most similar word to the mystery embedding\n",
    "most_similar_word = ...\n",
    "printmd(f\"The most similar word to the mystery embedding is: {most_similar_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve explored word-level embeddings, let’s move on to **document embeddings**. Instead of representing individual words, we’ll generate embeddings for entire IMDb reviews using **Doc2Vec**.\n",
    "\n",
    "</div>\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 1.4.3 </b>\n",
    "\n",
    "* Complete the code below to initialize and train [Gensim Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html) model. \n",
    "* Generate vector embeddings for the training documents.\n",
    "* What is the size of the embeddings?\n",
    "<div/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Tag the reviews\n",
    "tagged_reviews = [\n",
    "    TaggedDocument(words=review, tags=[idx])\n",
    "    for idx, review in enumerate(df_train_tokenized.review)\n",
    "]\n",
    "\n",
    "# Init Doc2Vec model\n",
    "print(\"Init Doc2Vec model...\")\n",
    "doc2vec_encoder = ...\n",
    "\n",
    "# Build vocabulary\n",
    "print(\"Building vocabulary...\")\n",
    "doc2vec_encoder.build_vocab(tagged_reviews)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "doc2vec_encoder.train(\n",
    "    tagged_reviews,\n",
    "    total_examples=doc2vec_encoder.corpus_count,\n",
    "    epochs=doc2vec_encoder.epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embedding vectors for the training set\n",
    "print(\"Infer vectors for the training set...\")\n",
    "X_train_doc2vec = np.array(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summing Up\n",
    "\n",
    "In this part, we explored various methods to vectorize our documents. Importantly, we ensured that the vocabulary was extracted, and the encoders were trained exclusively on the training data. This is crucial to avoid leaking information from the test set, which could lead to a biased evaluation of model performance.\n",
    "\n",
    "Let’s refactor the code to create a reusable function that utilizes the pre-fitted vectorizers. This function will be specifically useful later for vectorizing the test data, ensuring consistency in our evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(df_reviews, method):\n",
    "    \"\"\"\n",
    "    Vectorize the data using the specified method\n",
    "    Args:\n",
    "        df_reviews: pd.Series, reviews to vectorize\n",
    "        method: str, method to use for vectorization (\"bow\", \"tfidf\" or \"doc2vec\")\n",
    "    Returns:\n",
    "        X: np.array, vectorized data\n",
    "    \"\"\"\n",
    "    if method == \"bow\":\n",
    "        X = bow_encoder.transform(df_reviews)\n",
    "    elif method == \"tfidf\":\n",
    "        X = tfidf_encoder.transform(df_reviews)\n",
    "    elif method == \"doc2vec\":\n",
    "        X = np.array([doc2vec_encoder.infer_vector(review) for review in df_reviews])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Classification\n",
    "\n",
    "We have pre-fitted our vectorizers on the training data, it's time to apply them to the test set.\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 1.5.1 </b>\n",
    "\n",
    "* Ensure you are using clean tokenized test data before vectorization.\n",
    "* Vectorize the tokenized test data using the pre-fitted vectorizers for:\n",
    "  - Bag of Words (BoW)\n",
    "  - TF-IDF\n",
    "  - Doc2Vec\n",
    "\n",
    "* Assign the sentiment labels from the test data to a variable `y_test`.\n",
    "\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = {}\n",
    "\n",
    "print(\"BoW vectorization ...\")\n",
    "X_test[\"bow\"] = ...\n",
    "print(\"TF-IDF vectorization ...\")\n",
    "X_test[\"tfidf\"] = ...\n",
    "print(\"Doc2Vec vectorization ...\")\n",
    "X_test[\"doc2vec\"] = ...\n",
    "\n",
    "# Get the target\n",
    "y_test = df_test_tokenized.sentiment\n",
    "\n",
    "X_train = {\n",
    "    \"bow\": X_train_bow,\n",
    "    \"tfidf\": X_train_tfidf,\n",
    "    \"doc2vec\": X_train_doc2vec,\n",
    "}\n",
    "# Get the target\n",
    "y_train = df_train_tokenized.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b> Exercise 1.5.2 </b>\n",
    "\n",
    "- Train two classifier models: **Logistic Regression** and **Random Forest** for each of the following vectorization techniques:\n",
    "  - Bag of Words (BoW)\n",
    "  - TF-IDF\n",
    "  - Doc2Vec\n",
    "\n",
    "  This results in a total of **six models**. Organize these models in a nested dictionary structure, where:\n",
    "  - The first level of the dictionary is indexed by the **model name** (e.g., `\"logistic_regression\"`, `\"random_forest\"`).\n",
    "  - The second level is indexed by the **vectorization technique** (e.g., `\"bow\"`, `\"tfidf\"`, `\"doc2vec\"`).\n",
    "\n",
    "- Evaluate all trained models on the **test data**:\n",
    "  1. Compute the **accuracy** for each model.\n",
    "  2. Display the **confusion matrix** for each model to analyze their performance in detail.\n",
    "\n",
    "<div/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b> Exercise 1.5.3 </b>\n",
    "\n",
    "* Create a bar chart comparing the **accuracy** of all six models (use distinct colors to differentiate the vectorization techniques).\n",
    "* Identify the best combination of **classifier** and **vectorization technique** based on the highest accuracy and balanced performance across both classes.\n",
    "* Reflect: do the results align with your expectations?\n",
    "\n",
    "<div/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a38c21dc24837143ba33216130b9f8da84e53ab59c3fe234ca08dab4ce64daf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
