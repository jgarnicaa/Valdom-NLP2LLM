{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHw-MpQLC382"
      },
      "source": [
        "**All Rights Reserved**\n",
        "\n",
        "**Copyright (c) 2025 IRT Saint-Exupery**\n",
        "\n",
        "*Author & contact:*\n",
        "* mouhcine.mendil@irt-saintexupery.com\n",
        "\n",
        "# Natural Language Processing (NLP) to Large Language Models (LLM)\n",
        "\n",
        "<div align=\"center\">\n",
        "    <h2>Lab Session 1: Part II</h2>\n",
        "</div>\n",
        "\n",
        "## Machine Translation\n",
        "\n",
        "Our task is to automatically translate sentences from English to French. We will not perform a literal word-to-word translation, as the solution is a straighforward word retrieval from a lookup table. Instead, we aim to train the translation model by showing it several examples of English sentences and French sentences.  \n",
        "\n",
        "Machine Translation is an exemple of sequence-to-sequence learning (Seq2Seq), which consists on training models to convert **sequences of variable length** from one domain (e.g. sentences in English) to **sequences of variable length** in another domain (e.g. the same sentences translated to French). This can be used when you need to generate text, such as in machine translation or text summarization. There are multiple ways to handle this task; **We will focus on RNNs** that you have learned previously.\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "⚠️⚠️⚠️ Even if seq2seq models are suitable to handle variable-length sequences, they need to be trained on data with similar input sequence length $l_{\\text{input}}$ and output sequence length $l_{\\text{output}}$. We will see how to make this possible later in this notebook. ⚠️⚠️⚠️\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpLLXwlDC384"
      },
      "source": [
        "### 1. English-French MT Dataset\n",
        "\n",
        "We want to train a model to learn English to French translation from a simple dataset hosted in http://www.manythings.org/anki/. Besides, the website provides many translations for other languages such as English, Spanish and Chinese.\n",
        "\n",
        "We have previously download the dataset, which you can find in `data/eng_to_fr.txt`.\n",
        "\n",
        "<div class='alert alert-info'>\n",
        "\n",
        "<b> Exercise 1 </b>\n",
        "\n",
        "- Read in a dataframe the first 20000 rows of the file <code>data/eng_to_fr.txt</code>. Make sure you are using the right separator.\n",
        "- Get a general sense of what the dataset is about and describe it. Is the length of the input and target sequences similar ?\n",
        "- Keep only the first two columns and name them <code>english</code> and <code>french</code>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTqFwrGpC384",
        "outputId": "26bae476-df7b-49a0-de66-6cb47b86d20b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples: 5000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_mt = pd.read_csv(\"https://raw.githubusercontent.com/jgarnicaa/Valdom-NLP2LLM/refs/heads/main/lab%20projects/lab_session1/data/eng_to_fr.txt\", sep=\"\\t\", header=None)\n",
        "df_mt=df_mt.sample(n=5000, random_state=42).reset_index(drop=True)\n",
        "df_mt.drop(2, axis=1, inplace=True)\n",
        "df_mt.columns = [\"english\", \"french\"]\n",
        "print(f\"Number of samples: {len(df_mt)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHDh2ODqC385",
        "outputId": "a8c3a4bd-1449-47b9-e4c5-5efbd7ae63ca"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "english",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "french",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "e09d3da7-a184-4334-9260-1b483efc924d",
              "rows": [
                [
                  "0",
                  "Are you envious?",
                  "Êtes-vous jalouse ?"
                ],
                [
                  "1",
                  "All I want you to do is talk to us.",
                  "Tout ce que je veux que tu fasses est de nous parler."
                ],
                [
                  "2",
                  "You're a good student.",
                  "Vous êtes un bon étudiant."
                ],
                [
                  "3",
                  "Can you swim at all?",
                  "Sais-tu au moins nager ?"
                ],
                [
                  "4",
                  "He's curious about everything.",
                  "Il est curieux de tout."
                ],
                [
                  "5",
                  "I'll go back to Boston.",
                  "Je retournerai à Boston."
                ],
                [
                  "6",
                  "Hide in there.",
                  "Cache-toi là-dedans."
                ],
                [
                  "7",
                  "I'll explain the rules.",
                  "Je vais expliquer les règles."
                ],
                [
                  "8",
                  "We woke up very early in order to see the sunrise.",
                  "Nous nous levâmes tôt pour voir le lever du soleil."
                ],
                [
                  "9",
                  "Is that a promise?",
                  "S'agit-il d'une promesse ?"
                ],
                [
                  "10",
                  "Tom has narrow shoulders.",
                  "Tom a les épaules étroites."
                ],
                [
                  "11",
                  "I forgot what his name was.",
                  "J’ai oublié comment il s’appelle."
                ],
                [
                  "12",
                  "I wouldn't be so sure of that.",
                  "Je n'en serais pas si sûre."
                ],
                [
                  "13",
                  "You're beginning to scare me.",
                  "Tu commences à me faire peur."
                ],
                [
                  "14",
                  "I'm sorry I acted like a jerk.",
                  "Je suis désolé d'avoir agi comme un con."
                ],
                [
                  "15",
                  "Stop running.",
                  "Arrête de courir."
                ],
                [
                  "16",
                  "You look very pale.",
                  "Vous avez l'air très pâles."
                ],
                [
                  "17",
                  "Tom says that he never dreams.",
                  "Tom dit qu'il ne rêve jamais."
                ],
                [
                  "18",
                  "The problem should be solved by now.",
                  "Le problème devrait être résolu maintenant."
                ],
                [
                  "19",
                  "I'm happy to see you.",
                  "Je suis heureux de vous voir."
                ],
                [
                  "20",
                  "Not all of them are busy.",
                  "Tous ne sont pas occupés."
                ],
                [
                  "21",
                  "Smoke is rising from the chimney.",
                  "De la fumée s'échappe de la cheminée."
                ],
                [
                  "22",
                  "How long have your cheeks been swollen?",
                  "Depuis combien de temps vos joues sont-elles gonflées ?"
                ],
                [
                  "23",
                  "I was just trying to protect you.",
                  "J'essayais simplement de te protéger."
                ],
                [
                  "24",
                  "He crushed the sheet of paper up into a ball.",
                  "Il chiffonna la feuille de papier en une boule."
                ],
                [
                  "25",
                  "I'm sure that won't be necessary.",
                  "Je suis certain que ce ne sera pas nécessaire."
                ],
                [
                  "26",
                  "Do you have a cellphone?",
                  "Avez-vous un natel ?"
                ],
                [
                  "27",
                  "You're both wrong.",
                  "Vous avez tous les deux tort."
                ],
                [
                  "28",
                  "I do not like the way he talks.",
                  "Je déteste sa manière de parler."
                ],
                [
                  "29",
                  "Have you sent your novel to a publisher?",
                  "Est-ce que tu as envoyé ton roman à un éditeur ?"
                ],
                [
                  "30",
                  "I am playing the piano now.",
                  "Je suis en train de jouer du piano."
                ],
                [
                  "31",
                  "One of the tires was flat.",
                  "L'un des pneus était à plat."
                ],
                [
                  "32",
                  "Did you go to the restaurant yesterday?",
                  "Es-tu allé au restaurant hier ?"
                ],
                [
                  "33",
                  "I can't work today.",
                  "Je ne peux pas travailler aujourd'hui."
                ],
                [
                  "34",
                  "All I want is your love.",
                  "Tout ce que je veux, c'est votre affection."
                ],
                [
                  "35",
                  "You didn't tell me that you were a doctor.",
                  "Vous ne m'aviez pas dit que vous étiez docteur."
                ],
                [
                  "36",
                  "I was wondering the same thing.",
                  "Je me demandais la même chose."
                ],
                [
                  "37",
                  "Don't worry, I'm going to help you.",
                  "Ne vous faites pas de souci, je vais vous aider."
                ],
                [
                  "38",
                  "I'm sorry, I didn't mean that.",
                  "Je suis désolé, je ne voulais pas dire ça."
                ],
                [
                  "39",
                  "I thought that you'd already done that.",
                  "Je pensais que tu l'avais déjà fait."
                ],
                [
                  "40",
                  "There is a possibility that we won't have to shut down the factory.",
                  "Il y a une possibilité que nous n'ayons pas à fermer l'usine."
                ],
                [
                  "41",
                  "Who will you be coming with?",
                  "Avec qui viendrez-vous ?"
                ],
                [
                  "42",
                  "Did I mention that?",
                  "L'ai-je mentionné ?"
                ],
                [
                  "43",
                  "Have a good weekend.",
                  "Passe un bon week-end !"
                ],
                [
                  "44",
                  "I have some work to do this evening.",
                  "J'ai du travail à faire ce soir."
                ],
                [
                  "45",
                  "How far are you prepared to go?",
                  "Jusqu'où es-tu prête à aller ?"
                ],
                [
                  "46",
                  "We are fully aware of the importance of the situation.",
                  "Nous sommes pleinement conscients de l'importance de la situation."
                ],
                [
                  "47",
                  "I'm just trying to stop you from making a big mistake.",
                  "J'essaie juste de t'empêcher de commettre une grosse erreur."
                ],
                [
                  "48",
                  "I did that because everybody else did.",
                  "J'ai fait ça parce que tout le monde l'a fait."
                ],
                [
                  "49",
                  "I'll stay at home if it rains tomorrow.",
                  "Je resterai à la maison s'il pleut demain."
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 100
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>french</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Are you envious?</td>\n",
              "      <td>Êtes-vous jalouse ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>All I want you to do is talk to us.</td>\n",
              "      <td>Tout ce que je veux que tu fasses est de nous ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You're a good student.</td>\n",
              "      <td>Vous êtes un bon étudiant.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Can you swim at all?</td>\n",
              "      <td>Sais-tu au moins nager ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>He's curious about everything.</td>\n",
              "      <td>Il est curieux de tout.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>There appears to have been a mistake.</td>\n",
              "      <td>Il semble qu'il y ait eu là une erreur.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>He apologized to the employee.</td>\n",
              "      <td>Il s'excusa auprès de l'employée.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Come off it.</td>\n",
              "      <td>Arrête ton char.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Please deposit the money in a bank.</td>\n",
              "      <td>Déposez l'argent dans une banque s'il vous plait.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Tom came home very late.</td>\n",
              "      <td>Tom est rentré chez lui très tard.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  english  \\\n",
              "0                        Are you envious?   \n",
              "1     All I want you to do is talk to us.   \n",
              "2                  You're a good student.   \n",
              "3                    Can you swim at all?   \n",
              "4          He's curious about everything.   \n",
              "..                                    ...   \n",
              "95  There appears to have been a mistake.   \n",
              "96         He apologized to the employee.   \n",
              "97                           Come off it.   \n",
              "98    Please deposit the money in a bank.   \n",
              "99               Tom came home very late.   \n",
              "\n",
              "                                               french  \n",
              "0                                 Êtes-vous jalouse ?  \n",
              "1   Tout ce que je veux que tu fasses est de nous ...  \n",
              "2                          Vous êtes un bon étudiant.  \n",
              "3                            Sais-tu au moins nager ?  \n",
              "4                             Il est curieux de tout.  \n",
              "..                                                ...  \n",
              "95            Il semble qu'il y ait eu là une erreur.  \n",
              "96                  Il s'excusa auprès de l'employée.  \n",
              "97                                   Arrête ton char.  \n",
              "98  Déposez l'argent dans une banque s'il vous plait.  \n",
              "99                 Tom est rentré chez lui très tard.  \n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_mt.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRjLkZRKC385",
        "outputId": "0302ff8d-2c66-413e-f500-aa01fefdcc77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5000 entries, 0 to 4999\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   english  5000 non-null   object\n",
            " 1   french   5000 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 78.2+ KB\n"
          ]
        }
      ],
      "source": [
        "df_mt.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFO8oWv1C386"
      },
      "source": [
        "### 2. Data Cleaning and preparation\n",
        "<div class='alert alert-info'>\n",
        "<b> Exercise 2.1 </b>\n",
        "\n",
        "- To simplify the problem (smaller vocabulary), lower all capital letters.\n",
        "- Can we apply other data cleaning operations ? Explain your answer.\n",
        "- Split your data into training (80%) and test (20%) subsets using <code>train_test_split</code>, random seed = 42 and <code>shuffle</code> set to True.\n",
        "\n",
        "</div>\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "⚠️⚠️⚠️ Once done, note that vocabulary size and token index will be exclusively based on the train dataset. Make sure you choose the same random seed and other arguments specified in the question. ⚠️⚠️⚠️\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qflEjf0GC386",
        "outputId": "f275a404-5de2-4e65-a824-9896e1b457fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_28519/354221495.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df_mt_clean = df_mt.applymap(lambda s: s.lower() if type(s) == str else s)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# lower case\n",
        "df_mt_clean = df_mt.applymap(lambda s: s.lower() if type(s) == str else s)\n",
        "\n",
        "# Split data into train and test\n",
        "df_mt_train, df_mt_test = train_test_split(df_mt_clean, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "df_mt_train.reset_index(inplace=True, drop=True)\n",
        "df_mt_test.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOGG0EnGC386",
        "outputId": "7f526168-0dea-4057-e313-ec6526a02f36"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "english",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "french",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "f2fb3371-2f6b-45f3-9314-9cd7debc69e7",
              "rows": [
                [
                  "0",
                  "can i talk to you for a sec?",
                  "je peux vous parler une seconde ?"
                ],
                [
                  "1",
                  "i don't know anything about japan.",
                  "je ne connais rien du japon."
                ],
                [
                  "2",
                  "after six games, sampras had the edge on his opponent.",
                  "après six jeux, sampras prit l'avantage sur son rival."
                ],
                [
                  "3",
                  "you should run for president.",
                  "tu devrais te présenter en tant que président."
                ],
                [
                  "4",
                  "suddenly, it started to rain very hard.",
                  "soudain, il commença à pleuvoir très fort."
                ],
                [
                  "5",
                  "glad to see you, tom.",
                  "content de vous voir, tom."
                ],
                [
                  "6",
                  "when did you finish the work?",
                  "quand as-tu fini le travail ?"
                ],
                [
                  "7",
                  "she is no match for me.",
                  "elle n'est pas de taille avec moi."
                ],
                [
                  "8",
                  "i was tired from doing my homework.",
                  "j'en avais marre de faire mes devoirs."
                ],
                [
                  "9",
                  "it's an excellent wine.",
                  "c'est un excellent vin."
                ],
                [
                  "10",
                  "come quickly!",
                  "dépêche-toi de venir !"
                ],
                [
                  "11",
                  "it's going to be close.",
                  "ça va être serré."
                ],
                [
                  "12",
                  "don't leave this room.",
                  "ne quitte pas cette pièce."
                ],
                [
                  "13",
                  "he didn't have a single pen.",
                  "il ne disposait d'aucun stylo."
                ],
                [
                  "14",
                  "i'm glad to be the one who tells you.",
                  "je me réjouis d'être celle qui te le dit."
                ],
                [
                  "15",
                  "you understand this, don't you?",
                  "tu comprends ceci, n'est-ce pas ?"
                ],
                [
                  "16",
                  "tom came home very late.",
                  "tom est rentré chez lui très tard."
                ],
                [
                  "17",
                  "he won the prize last week.",
                  "il a gagné le prix la semaine passée."
                ],
                [
                  "18",
                  "that was horrible.",
                  "ça a été horrible."
                ],
                [
                  "19",
                  "no one in my family can do that.",
                  "dans ma famille, personne n'est capable de faire ça."
                ],
                [
                  "20",
                  "in that case, i'll come.",
                  "dans ce cas, je viens."
                ],
                [
                  "21",
                  "the man was hiding in a dense forest.",
                  "l'homme se cachait dans une forêt dense."
                ],
                [
                  "22",
                  "it's not the answer.",
                  "ce n'est pas la réponse."
                ],
                [
                  "23",
                  "hitler assumed power in 1933.",
                  "hitler a pris le pouvoir en 1933."
                ],
                [
                  "24",
                  "i feel honored.",
                  "je me sens honorée."
                ],
                [
                  "25",
                  "tom kept coughing.",
                  "tom continuait à tousser."
                ],
                [
                  "26",
                  "i don't know whether it's important to you, but it's very important to me.",
                  "je ne sais pas si c'est important pour vous mais c'est très important pour moi."
                ],
                [
                  "27",
                  "tom isn't able to understand french.",
                  "tom ne comprend pas le français."
                ],
                [
                  "28",
                  "go say goodbye to tom.",
                  "va dire au revoir à tom."
                ],
                [
                  "29",
                  "you can't put off doing that any longer.",
                  "tu ne peux plus le postposer."
                ],
                [
                  "30",
                  "you guys are right, of course.",
                  "vous avez raison, bien sûr."
                ],
                [
                  "31",
                  "when tom stopped for a stop sign, his engine stalled.",
                  "lorsque tom s'est arrêté au stop, son engin a calé."
                ],
                [
                  "32",
                  "keep in mind that smoking is not good for your health.",
                  "garde à l'esprit que fumer n'est pas bon pour ta santé."
                ],
                [
                  "33",
                  "what kind of music do you like?",
                  "quelle sorte de musique aimes-tu ?"
                ],
                [
                  "34",
                  "i think it's time for me to clean the chimney.",
                  "je pense qu'il est temps que je nettoie la cheminée."
                ],
                [
                  "35",
                  "i said he could go.",
                  "je dis qu'il pouvait y aller."
                ],
                [
                  "36",
                  "don't you want to know what i want?",
                  "ne veux-tu pas savoir ce que je veux ?"
                ],
                [
                  "37",
                  "i'm not the one who should go.",
                  "je ne suis pas celle qui devrait y aller."
                ],
                [
                  "38",
                  "you like that, don't you?",
                  "tu aimes ça, hein ?"
                ],
                [
                  "39",
                  "the last thing i want to do is cause you any more pain.",
                  "la dernière chose que je veuille faire est vous causer le moindre mal supplémentaire."
                ],
                [
                  "40",
                  "there's not a big difference.",
                  "il n'y a pas une grande différence."
                ],
                [
                  "41",
                  "my brother is a first-year student.",
                  "mon frère est étudiant de première année."
                ],
                [
                  "42",
                  "stand up and introduce yourself, please.",
                  "lève-toi et présente-toi, s'il te plait."
                ],
                [
                  "43",
                  "this is kind of embarrassing.",
                  "c'est assez embarrassant."
                ],
                [
                  "44",
                  "that's all i needed.",
                  "c'est tout ce dont j'avais besoin."
                ],
                [
                  "45",
                  "you're very tall.",
                  "tu es très grande."
                ],
                [
                  "46",
                  "tom will pay you all the money i owe you. \"you'd better hope so.\"",
                  "« tom vous paiera tout l'argent que je vous dois. » « il vaudrait mieux pour vous que ce soit le cas. »"
                ],
                [
                  "47",
                  "tom overslept.",
                  "tom s'est réveillé trop tard."
                ],
                [
                  "48",
                  "i have a previous engagement.",
                  "je suis déjà pris."
                ],
                [
                  "49",
                  "i don't understand this at all.",
                  "je ne comprends pas du tout ça."
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 4000
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>french</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>can i talk to you for a sec?</td>\n",
              "      <td>je peux vous parler une seconde ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i don't know anything about japan.</td>\n",
              "      <td>je ne connais rien du japon.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>after six games, sampras had the edge on his o...</td>\n",
              "      <td>après six jeux, sampras prit l'avantage sur so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>you should run for president.</td>\n",
              "      <td>tu devrais te présenter en tant que président.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>suddenly, it started to rain very hard.</td>\n",
              "      <td>soudain, il commença à pleuvoir très fort.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3995</th>\n",
              "      <td>i can't do two things at the same time.</td>\n",
              "      <td>je ne parviens pas à faire deux choses à la fois.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3996</th>\n",
              "      <td>let's not forget that.</td>\n",
              "      <td>ne l'oublions pas !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3997</th>\n",
              "      <td>don't worry. i'm not going anywhere.</td>\n",
              "      <td>ne te fais pas de souci. je ne vais nulle part.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3998</th>\n",
              "      <td>i'm not free to go this afternoon.</td>\n",
              "      <td>je ne suis pas libre de mes mouvements cet apr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3999</th>\n",
              "      <td>i just want to let you know that i can't atten...</td>\n",
              "      <td>je veux juste te faire savoir que je ne peux p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                english  \\\n",
              "0                          can i talk to you for a sec?   \n",
              "1                    i don't know anything about japan.   \n",
              "2     after six games, sampras had the edge on his o...   \n",
              "3                         you should run for president.   \n",
              "4               suddenly, it started to rain very hard.   \n",
              "...                                                 ...   \n",
              "3995            i can't do two things at the same time.   \n",
              "3996                             let's not forget that.   \n",
              "3997               don't worry. i'm not going anywhere.   \n",
              "3998                 i'm not free to go this afternoon.   \n",
              "3999  i just want to let you know that i can't atten...   \n",
              "\n",
              "                                                 french  \n",
              "0                     je peux vous parler une seconde ?  \n",
              "1                          je ne connais rien du japon.  \n",
              "2     après six jeux, sampras prit l'avantage sur so...  \n",
              "3        tu devrais te présenter en tant que président.  \n",
              "4            soudain, il commença à pleuvoir très fort.  \n",
              "...                                                 ...  \n",
              "3995  je ne parviens pas à faire deux choses à la fois.  \n",
              "3996                                ne l'oublions pas !  \n",
              "3997    ne te fais pas de souci. je ne vais nulle part.  \n",
              "3998  je ne suis pas libre de mes mouvements cet apr...  \n",
              "3999  je veux juste te faire savoir que je ne peux p...  \n",
              "\n",
              "[4000 rows x 2 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_mt_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_xDwf1mC387"
      },
      "source": [
        "To implement Machine Translation, we will rely on sequence-to-sequence (Seq2Seq) models.\n",
        "\n",
        "### Seq2Seq: Encoder/Decoder\n",
        "\n",
        "Seq2Seq has been first introduced by [Google](https://arxiv.org/abs/1409.3215). It captures the information carried by the input sequences in a low-dimensional encoded form and generates relevant output sequences in an iterative manner. Seq2Seq relies on two main blocks: one to read the input sequence (encoder) and another to generate the output sequence (decoder).\n",
        "\n",
        "**Encoder**  \n",
        "The encoder processes the input sequence. It reads the input data **one token at a time** and transform it into a **context vector** (fixed-sized vector), capturing the data's essential information. The encoder (RNN, LSTM, or GRU) traverses the input sequence, updating its internal state at each step. By the end of this process, the internal state of the encoder is a compact representation of the entire input sequence.\n",
        "\n",
        "**Decoder**  \n",
        "The decoder is tasked with generating the output sequence. Starting from the **context vector** produced by the encoder, it generates the output elements one at a time. Like the encoder, the decoder is often implemented as an RNN, LSTM, or GRU. It uses the context vector and what it has generated so far to predict the next element in the output sequence. This process is iterative and continues until a special **end-of-sequence token** is generated (or some other stopping criterion).\n",
        "\n",
        "At each decoding step, the output of the decoder is passed through a **dense layer followed by a softmax function**. The softmax produces a probability distribution over the entire **target vocabulary**, allowing the model to select the most likely next token. This softmax layer is crucial, as it transforms the decoder’s output into a meaningful prediction at every timestep.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/jgarnicaa/Valdom-NLP2LLM/blob/main/lab%20projects/lab_session1/figures/seq2seqmodel.png?raw=1\" width=\"80%\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD8a0QBfC387"
      },
      "source": [
        "----\n",
        "\n",
        "### Tokenization\n",
        "\n",
        "Now we know the type of model we will use, let's move to tokenization. We will rely on character-to-character machine translation; therefore, sentences will be tokenized at <b>character-level</b>.\n",
        "\n",
        "Note however that there are two additional special tokens that we need for Seq2Seq models: the Start-of-Sequence (SOS) token and the End-of-Sequence (EOS).\n",
        "\n",
        "- The SOS is essential for initializing the decoding process. During training, it serves as the first input to the decoder, signaling the model to begin generating the target sequence. This helps the model learn a consistent starting context across all examples. During inference (i.e., when generating a translation), the SOS token is explicitly fed as the first decoder input, which triggers the model to begin producing the output sequence.\n",
        "\n",
        "- The End-of-Sequence (EOS) token, on the other hand, indicates when the generated sequence should stop. It is included in the target output during training so the model can learn to associate certain contexts with the end of a sentence. During inference, the model continues generating tokens until it predicts an EOS token, which signals that the output is complete. Without EOS, the model might continue generating irrelevant tokens or cut off the translation prematurely. Together, SOS and EOS provide clear boundaries for structured and meaningful sequence generation.\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/jgarnicaa/Valdom-NLP2LLM/blob/main/lab%20projects/lab_session1/figures/tokenization.png?raw=1\" width=\"40%\"/>\n",
        "  <figcaption>Author: Shann Khosla<figcaption/>\n",
        "</div>\n",
        "\n",
        "<div class='alert alert-info'>\n",
        "<b> Exercise 2.2 </b>\n",
        "\n",
        "- Add a Start-Of-Sentence (SOS) characters `\\t` and End-Of-Sentence (EOS) character `\\n` to <b>each french sentence (target)</b>. For example, \"bonjour!\" becomes \"\\tbonjour!\\n\"\n",
        "- What is the maximum sequence length for input text (English) and target text (French) ?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8DBnnQkC387",
        "outputId": "4f11e412-d3d5-4728-97df-a743f2f7042d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of characters per text for inputs (English): 164\n",
            "Max number of characters per text for outputs (French): 178\n"
          ]
        }
      ],
      "source": [
        "#Adding SOS and EOS tokens\n",
        "df_mt_train.french = df_mt_train.french.apply(lambda x: '\\t' + x + '\\n')\n",
        "df_mt_test.french = df_mt_test.french.apply(lambda x: '\\t' + x + '\\n')\n",
        "\n",
        "# Max seq lenght for english and french\n",
        "max_fr_seq_len = df_mt_train.french.str.len().max()\n",
        "max_en_seq_len = df_mt_train.english.str.len().max()\n",
        "\n",
        "print(f\"Max number of characters per text for inputs (English): {max_en_seq_len}\")\n",
        "print(f\"Max number of characters per text for outputs (French): {max_fr_seq_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnQjG6OaC388"
      },
      "source": [
        "<div class='alert alert-info'>\n",
        "\n",
        "<b> Exercise 2.3 </b>\n",
        "- Build a vocabulary for the input text (English) and a vocabulary for the target text (French) using only <b>train dataset</b>. What is the size of each vocabulary ?\n",
        "- Build an input token-to-index dictionary to map each character (key) from the previously constructed English vocabulary to a unique index (value).\n",
        "- Build a target token-to-index dictionary to map each character (key) from the previously constructed French vocabulary to a unique index (value).\n",
        "Make sure that the character <code>\\t</code> is associated with index 0 and <code>\\n</code> is associated with index 1.     \n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rad7of6OC388",
        "outputId": "8b8b0251-c141-4c54-abea-b97d6a298e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of English vocabulary: 46\n",
            "Size of French vocabulary: 68\n"
          ]
        }
      ],
      "source": [
        "# Vocabulary\n",
        "vocab_en = []\n",
        "for text in df_mt_train.english:\n",
        "    for char in text:\n",
        "        if char not in vocab_en:\n",
        "            vocab_en.append(char)\n",
        "vocab_en.sort()\n",
        "vocab_fr = []\n",
        "for text in df_mt_train.french:\n",
        "    for char in text:\n",
        "        if char not in vocab_fr:\n",
        "            vocab_fr.append(char)\n",
        "vocab_fr.sort()\n",
        "print(\"Size of English vocabulary:\", len(vocab_en))\n",
        "print(\"Size of French vocabulary:\", len(vocab_fr))\n",
        "\n",
        "# Token to index lookup dicts for english and french\n",
        "en_token2index_dict = {token: idx for idx, token in enumerate(vocab_en)}\n",
        "fr_token2index_dict = {token: idx for idx, token in enumerate(vocab_fr)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk7d0kB4C389",
        "outputId": "9c3caa9d-0af1-4827-fb5f-7642bf4f58b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fr_token2index_dict['\\t']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS7ZS0fKC389"
      },
      "source": [
        "### Training vs. Inference in Seq2Seq Models\n",
        "\n",
        "Although the architecture of a Seq2Seq model (encoder + decoder) stays the same, its **behavior during training and inference is very different** — and it’s important to understand why.\n",
        "\n",
        "##### **Training Phase** – *Using the Right Answer*\n",
        "\n",
        "During training, we use a method called **teacher forcing**. This means that at each time step, the decoder is given the **correct previous token** from the target sentence (french) — not the one the model predicted.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/jgarnicaa/Valdom-NLP2LLM/blob/main/lab%20projects/lab_session1/figures/teacher_forcing.png?raw=1\" width=\"40%\"/>\n",
        "  <figcaption>Author: Wanshun Wong<figcaption/>\n",
        "</div>\n",
        "\n",
        "At each step, the model learns to predict the **next token** given the **true previous one**. This makes training faster and more stable because the decoder always knows the \"right context,\" even if it would have made a mistake on its own.\n",
        "\n",
        "**The model is trained to minimize the difference between its predicted output sequence and the true output sequence, using a loss function like cross-entropy.**\n",
        "\n",
        "##### **Inference Phase** – *Using Its Own Predictions*\n",
        "\n",
        "When we switch to inference (i.e., generating a translation), the model doesn’t have access to the ground-truth translation anymore. It has to **generate the output one token at a time**, feeding **its own previous prediction** back into the decoder.\n",
        "\n",
        "It starts with the **start-of-sequence token (SOS)** and continues predicting the next token based on everything it has generated so far. It stops when it outputs the **end-of-sequence token (EOS)** or hits a maximum sequence length.\n",
        "\n",
        "This process is called **autoregressive decoding**, and it's harder because:\n",
        "- There is no ground truth to guide the model\n",
        "- One small error early on can affect all the next predictions\n",
        "\n",
        "##### Summary\n",
        "\n",
        "| Phase     | What is fed to the decoder?          | Purpose                            |\n",
        "|-----------|--------------------------------------|------------------------------------|\n",
        "| Training  | The correct previous token (teacher forcing) | To help the model learn faster and more accurately |\n",
        "| Inference | The model’s own previous prediction  | To test whether the model can generate coherent sequences by itself |\n",
        "\n",
        "Understanding this distinction is key: **teacher forcing helps the model learn**, while **inference checks whether it really has**.\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "Knowing this, let's proceed to the vectorization using one-hot encoding.\n",
        "\n",
        "<div class='alert alert-info'>\n",
        "<b> Exercise 2.4 </b>\n",
        "\n",
        "- Write a function `one_hot_input` that prepares:\n",
        "    - the encoder's inputs `encoder_one_hot_inputs` as a 3D array of shape `(size_corpus, max_english_sentence_length, size_english_vocabulary)` containing the one-hot vectorization of the English sentences. The sentences shorter than `max_english_sentence_length` are to be filled with spaces `\" \"` (padding). Replace out-of-vocabulary tokens by a space `\" \"`.\n",
        "    - the decoder's inputs `decoder_one_hot_inputs` as a 3D array of shape `(size_corpus, max_french_sentence_length, size_french_vocabulary)` containing the one-hot vectorization of the French sentences. The sentences shorter than `max_french_sentence_length` are to be filled with spaces `\" \"`. Replace out-of-vocabulary tokens by a space `\" \"`.\n",
        "</div>\n",
        "\n",
        "Consistently with the teacher forcing approach, notice how the function `one_hot_target` that prepares the decoder's target `decoder_one_hot_targets` is the same as `decoder_one_hot_inputs` but offset by one step: `decoder_one_hot_targets[:, j, :]` $\\leftarrow$ `decoder_one_hot_inputs[:, j + 1, :]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZabZiMfxC389",
        "outputId": "a2fe7d91-0f55-44f1-f2d5-f42279826801"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4000"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_mt_train.english)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB3SgwYdC38-"
      },
      "outputs": [],
      "source": [
        "def one_hot_input(corpus, token2index_dict, max_seq_len):\n",
        "    result = np.zeros((len(corpus), max_seq_len, len(token2index_dict)), dtype=\"float32\")\n",
        "    for i, document in enumerate(corpus):\n",
        "        for t, char in enumerate(document):\n",
        "            try:\n",
        "                result[i, t, token2index_dict[char]] = 1\n",
        "                #padding with space if length exceeded\n",
        "            except:\n",
        "                result[i, t, token2index_dict[\" \"]] = 1\n",
        "                continue\n",
        "        result[i, t + 1 :, token2index_dict[\" \"]] = 1.0\n",
        "    return result\n",
        "\n",
        "\n",
        "def one_hot_target(corpus, token2index_dict, max_seq_len):\n",
        "    result = np.zeros(\n",
        "        (len(corpus), max_seq_len, len(token2index_dict)), dtype=\"float32\"\n",
        "    )\n",
        "    for i, document in enumerate(corpus):\n",
        "        for t, char in enumerate(document):\n",
        "            if t > 0:\n",
        "                try:\n",
        "                    result[i, t - 1, token2index_dict[char]] = 1\n",
        "                except:\n",
        "                    result[i, t - 1, token2index_dict[\" \"]] = 1\n",
        "                    continue\n",
        "        result[i, t:, token2index_dict[\" \"]] = 1\n",
        "    return result\n",
        "\n",
        "\n",
        "encoder_one_hot_inputs = one_hot_input(\n",
        "    df_mt_train.english, en_token2index_dict, max_en_seq_len\n",
        ")\n",
        "decoder_one_hot_inputs = one_hot_input(\n",
        "    df_mt_train.french, fr_token2index_dict, max_fr_seq_len\n",
        ")\n",
        "decoder_one_hot_targets = one_hot_target(\n",
        "    df_mt_train.french, fr_token2index_dict, max_fr_seq_len\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjYE4GXpC38-"
      },
      "source": [
        "### 3. Model Training\n",
        "\n",
        "\n",
        "We will now implement the architecture shown in the figure below to train a Seq2Seq model for character-level machine translation.\n",
        "\n",
        "On the left, the encoder processes a sequence of one-hot encoded input characters (from an English sentence) using a stack of GRU cells. As the encoder reads each character, it updates its internal hidden state. Once the final character is processed, the last hidden state becomes the context vector summarizing the entire input sentence.\n",
        "\n",
        "This context vector is then passed to the decoder on the right. The decoder is also composed of GRU cells, and at each time step, it receives:\n",
        "\n",
        "* The one-hot encoded representation of the previous character (from the target sentence during training, or from its own prediction during inference)\n",
        "\n",
        "* The current hidden state (initially, the final hidden state from the encoder)\n",
        "\n",
        "The decoder's GRU outputs are passed through a Dense layer followed by a softmax activation, producing a probability distribution over the target vocabulary at each time step. This allows the model to predict the next character in the output sequence. The decoder continues this autoregressive process until it generates a special end-of-sequence (EOS) token.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/jgarnicaa/Valdom-NLP2LLM/blob/main/lab%20projects/lab_session1/figures/seq2seqmodel.png?raw=1\" width=\"80%\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "<div class='alert alert-info'>\n",
        "<b> Exercise 3.1 </b>\n",
        "\n",
        "- Briefly explain how a GRU layer works: What are its inputs, outputs, and why is it useful in the Seq2Seq context?\n",
        "- Examine and complement the implementation of the `seq2seq_model` function to build the Seq2Seq model.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ7H_4o6C38-",
        "outputId": "2d50c237-fe77-46c4-dd2d-4af7d741352c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-03 15:59:05.896847: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-02-03 15:59:06.347100: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2026-02-03 15:59:08.797658: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def seq2seq_model(latent_dim, input_tokenidx_dict, target_tokenidx_dict):\n",
        "    # Define an input sequence and process it.\n",
        "    input_vocab_size = len(input_tokenidx_dict)\n",
        "    target_vocab_size = len(target_tokenidx_dict)\n",
        "\n",
        "    # 1. Encoder\n",
        "    x_encoder = tf.keras.Input(shape=(None, input_vocab_size))\n",
        "    encoder = tf.keras.layers.GRU(latent_dim, return_state=True)\n",
        "    y_encoder, h_encoder = encoder(x_encoder)\n",
        "\n",
        "    # 2. Decoder\n",
        "    x_decoder = tf.keras.Input(shape=(None, target_vocab_size))\n",
        "    decoder = tf.keras.layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "    y_decoder, _ = decoder(x_decoder, initial_state=h_encoder)\n",
        "\n",
        "    # 3. Output\n",
        "    dense_softmax = tf.keras.layers.Dense(target_vocab_size, activation=\"softmax\")\n",
        "    y_decoder = dense_softmax(y_decoder)\n",
        "\n",
        "    model = tf.keras.Model([x_encoder, x_decoder], y_decoder)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aombBbnYC38_"
      },
      "source": [
        "<div class='alert alert-info'>\n",
        "<b> Exercise 3.2 </b>\n",
        "\n",
        "- Instantiate and train the Seq2Seq model using the teacher forcing approach.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZL5VBO1C39H",
        "outputId": "b7279eb9-f01e-424a-9f43-d762e50acd2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-03 15:59:09.644391: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2026-02-03 15:59:10.230148: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 96563200 exceeds 10% of free system memory.\n",
            "2026-02-03 15:59:10.538948: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 154931200 exceeds 10% of free system memory.\n",
            "2026-02-03 15:59:10.913304: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 154931200 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.7795 - loss: 1.3092"
          ]
        }
      ],
      "source": [
        "batch_size = 32  # Batch size for training.\n",
        "epochs = 10  # Number of epochs to train for.\n",
        "latent_dim = 512  # dim of the latent space.\n",
        "\n",
        "# Instantiate the model and compile it\n",
        "model = seq2seq_model(latent_dim, en_token2index_dict, fr_token2index_dict)\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Set early stopping if validation accuracy does not improve after `patience` epochs\n",
        "callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=50, restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train\n",
        "history = model.fit(\n",
        "    [encoder_one_hot_inputs, decoder_one_hot_inputs],  # inputs\n",
        "    decoder_one_hot_targets,  # targets\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX3ZigwvC39H"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKgOahffC39I"
      },
      "source": [
        "### 4. Model Inference\n",
        "\n",
        "Unlike training, the target sequence is not known during inference. The model generates the output sequence character by character, starting from an SOS character until it generates an EOS character, signifying the end of the output sequence. Since the true output tokens are not available, the model uses its own predictions as input for the next step. This process is autoregressive and continues until the model produces the EOS or reaches a maximum length.\n",
        "\n",
        "During inference, the model generates the target sequence character by character, in an autoregressive way.\n",
        "\n",
        "<div class='alert alert-info'>\n",
        "<b> Exercise 4.1 </b>\n",
        "\n",
        "- Examine the implementation of the `translate` function and explain how the Seq2Seq model is used for inference.\n",
        "- Run the inference on some test sentences one by one.\n",
        "- What do you notice about errors with this autoregressive generation? Are mistakes equally distributed throughout a sequence (begining, middle and end)?\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yy7UttiaC39I"
      },
      "outputs": [],
      "source": [
        "def translate(input_seq, seq2seq_model=model):\n",
        "    # You can check seq2seq_model.summary() to recall the input and outputs\n",
        "    # of its layers\n",
        "\n",
        "    # Encoder\n",
        "    x_encoder = seq2seq_model.input[0]  # input_1\n",
        "    y_encoder, h_encoder = seq2seq_model.layers[2].output  # lstm_1\n",
        "    encoder = tf.keras.Model(inputs=x_encoder, outputs=h_encoder)\n",
        "\n",
        "    # Decoder\n",
        "    x_decoder = seq2seq_model.input[1]  # input_2\n",
        "    h_in_decoder = tf.keras.Input(shape=(latent_dim,))\n",
        "    gru_layer = seq2seq_model.layers[3]\n",
        "    y_gru, h_out_gru = gru_layer(x_decoder, initial_state=h_in_decoder)\n",
        "    dense_layer = seq2seq_model.layers[4]\n",
        "    y_decoder = dense_layer(y_gru)\n",
        "    decoder_model = tf.keras.Model([x_decoder, h_in_decoder], [y_decoder, h_out_gru])\n",
        "\n",
        "    # Reverse dictionary to recover target vocabulary from token index\n",
        "    token_idx_fr_dict = dict((i, char) for char, i in fr_token2index_dict.items())\n",
        "\n",
        "    def decode_translation(input_seq):\n",
        "        # Encode input as context vector.\n",
        "        h_encoder = encoder.predict(input_seq, verbose=0)\n",
        "\n",
        "        # Generate empty target sequence of length 1.\n",
        "        translated_seq = np.zeros((1, 1, len(fr_token2index_dict)))\n",
        "        # Initialize the first character of target sequence with the SOS.\n",
        "        translated_seq[0, 0, fr_token2index_dict[\"\\t\"]] = 1.0\n",
        "\n",
        "        # Sampling loop for a batch (=1) of sequences\n",
        "        condition = False\n",
        "        decoded_sentence = \"\"\n",
        "        h_in_decoder = h_encoder  # init\n",
        "        while not condition:\n",
        "            y_decoder, h_out_decoder = decoder_model.predict(\n",
        "                [translated_seq, h_in_decoder], verbose=0\n",
        "            )\n",
        "\n",
        "            # Get token\n",
        "            token_index = np.argmax(y_decoder[0, -1, :])\n",
        "            char = token_idx_fr_dict[token_index]\n",
        "            decoded_sentence += char\n",
        "\n",
        "            # Stop condition: find stop character or reach max length\n",
        "            if char == \"\\n\" or len(decoded_sentence) > max_fr_seq_len:\n",
        "                condition = True\n",
        "\n",
        "            # Update the translated sequence.\n",
        "            translated_seq = np.zeros((1, 1, len(fr_token2index_dict)))\n",
        "            translated_seq[0, 0, token_index] = 1.0\n",
        "\n",
        "            # Update hidden state\n",
        "            h_in_decoder = h_out_decoder\n",
        "        return decoded_sentence\n",
        "\n",
        "    return decode_translation(input_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcI-w0bzC39I"
      },
      "outputs": [],
      "source": [
        "# test input sequences\n",
        "input_sequences = ...\n",
        "\n",
        "\n",
        "for i, seq_index in enumerate(range(20)):\n",
        "\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = input_sequences[seq_index : seq_index + 1]\n",
        "    # encoder_one_hot_inputs[seq_index : seq_index + 1]\n",
        "    translated_sentence = ...\n",
        "\n",
        "    print(f\"-------------------- Sentence {i} --------------------\")\n",
        "    print(\"Input sentence:\", df_mt_test.english[seq_index])\n",
        "    print(\"Translated sentence:\", translated_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXJ2a6QC39J"
      },
      "source": [
        "BLEU score is one of the most common metrics used to evaluate the quality of machine translation models. It measures how closely the model’s output matches one or more reference translations.\n",
        "\n",
        "<div class='alert alert-info'>\n",
        "<b> Exercise 4.2 </b>\n",
        "\n",
        "* Briefly explain how the BLEU score works. What does it measure, and how is it computed?\n",
        "\n",
        "* Using `nltk.translate.bleu_score`, compute the average BLEU score over at least 100 sentence pairs (predicted vs. reference). Interpret the result: what does it say about the model's performance?\n",
        "\n",
        "* Suggest some ways to improve the performance. For example, you can consider data quality, preparation, vectorization or model architecture.\n",
        "\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}