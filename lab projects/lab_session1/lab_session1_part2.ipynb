{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All Rights Reserved**\n",
    "\n",
    "**Copyright (c) 2025 IRT Saint-Exupery**\n",
    "\n",
    "*Author & contact:* \n",
    "* mouhcine.mendil@irt-saintexupery.com \n",
    "\n",
    "# Natural Language Processing (NLP) to Large Language Models (LLM)\n",
    "\n",
    "<div align=\"center\">\n",
    "    <h2>Lab Session 1: Part II</h2>\n",
    "</div>\n",
    "\n",
    "## Machine Translation\n",
    "\n",
    "Our task is to automatically translate sentences from English to French. We will not perform a literal word-to-word translation, as the solution is a straighforward word retrieval from a lookup table. Instead, we aim to train the translation model by showing it several examples of English sentences and French sentences.  \n",
    "\n",
    "Machine Translation is an exemple of sequence-to-sequence learning (Seq2Seq), which consists on training models to convert **sequences of variable length** from one domain (e.g. sentences in English) to **sequences of variable length** in another domain (e.g. the same sentences translated to French). This can be used when you need to generate text, such as in machine translation or text summarization. There are multiple ways to handle this task; **We will focus on RNNs** that you have learned previously.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "⚠️⚠️⚠️ Even if seq2seq models are suitable to handle variable-length sequences, they need to be trained on data with similar input sequence length $l_{\\text{input}}$ and output sequence length $l_{\\text{output}}$. We will see how to make this possible later in this notebook. ⚠️⚠️⚠️\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. English-French MT Dataset\n",
    "\n",
    "We want to train a model to learn English to French translation from a simple dataset hosted in http://www.manythings.org/anki/. Besides, the website provides many translations for other languages such as English, Spanish and Chinese. \n",
    "\n",
    "We have previously download the dataset, which you can find in `data/eng_to_fr.txt`.\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "\n",
    "<b> Exercise 1 </b>\n",
    "\n",
    "- Read in a dataframe the first 20000 rows of the file <code>data/eng_to_fr.txt</code>. Make sure you are using the right separator.\n",
    "- Get a general sense of what the dataset is about and describe it. Is the length of the input and target sequences similar ?\n",
    "- Keep only the first two columns and name them <code>english</code> and <code>french</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 232736\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_mt = pd.read_csv(\"data/eng_to_fr.txt\", sep=\"\\t\", header=None)\n",
    "df_mt.drop(2, axis=1, inplace=True)\n",
    "df_mt.columns = [\"english\", \"french\"]\n",
    "print(f\"Number of samples: {len(df_mt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "english",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "french",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e1d02cf8-95b5-4258-ba98-f8ae92dd07ca",
       "rows": [
        [
         "0",
         "Go.",
         "Va !"
        ],
        [
         "1",
         "Go.",
         "Marche."
        ],
        [
         "2",
         "Go.",
         "En route !"
        ],
        [
         "3",
         "Go.",
         "Bouge !"
        ],
        [
         "4",
         "Hi.",
         "Salut !"
        ],
        [
         "5",
         "Hi.",
         "Salut."
        ],
        [
         "6",
         "Run!",
         "Cours !"
        ],
        [
         "7",
         "Run!",
         "Courez !"
        ],
        [
         "8",
         "Run!",
         "Prenez vos jambes à vos cous !"
        ],
        [
         "9",
         "Run!",
         "File !"
        ],
        [
         "10",
         "Run!",
         "Filez !"
        ],
        [
         "11",
         "Run!",
         "Cours !"
        ],
        [
         "12",
         "Run!",
         "Fuyez !"
        ],
        [
         "13",
         "Run!",
         "Fuyons !"
        ],
        [
         "14",
         "Run.",
         "Cours !"
        ],
        [
         "15",
         "Run.",
         "Courez !"
        ],
        [
         "16",
         "Run.",
         "Prenez vos jambes à vos cous !"
        ],
        [
         "17",
         "Run.",
         "File !"
        ],
        [
         "18",
         "Run.",
         "Filez !"
        ],
        [
         "19",
         "Run.",
         "Cours !"
        ],
        [
         "20",
         "Run.",
         "Fuyez !"
        ],
        [
         "21",
         "Run.",
         "Fuyons !"
        ],
        [
         "22",
         "Who?",
         "Qui ?"
        ],
        [
         "23",
         "Wow!",
         "Ça alors !"
        ],
        [
         "24",
         "Wow!",
         "Waouh !"
        ],
        [
         "25",
         "Wow!",
         "Wah !"
        ],
        [
         "26",
         "Duck!",
         "À terre !"
        ],
        [
         "27",
         "Duck!",
         "Baisse-toi !"
        ],
        [
         "28",
         "Duck!",
         "Baissez-vous !"
        ],
        [
         "29",
         "Fire!",
         "Au feu !"
        ],
        [
         "30",
         "Help!",
         "À l'aide !"
        ],
        [
         "31",
         "Hide.",
         "Cache-toi."
        ],
        [
         "32",
         "Hide.",
         "Cachez-vous."
        ],
        [
         "33",
         "Jump!",
         "Saute."
        ],
        [
         "34",
         "Jump.",
         "Saute."
        ],
        [
         "35",
         "Stop!",
         "Ça suffit !"
        ],
        [
         "36",
         "Stop!",
         "Stop !"
        ],
        [
         "37",
         "Stop!",
         "Arrête-toi !"
        ],
        [
         "38",
         "Wait!",
         "Attends !"
        ],
        [
         "39",
         "Wait!",
         "Attendez !"
        ],
        [
         "40",
         "Wait!",
         "Attendez."
        ],
        [
         "41",
         "Wait.",
         "Attends !"
        ],
        [
         "42",
         "Wait.",
         "Attendez !"
        ],
        [
         "43",
         "Wait.",
         "Attends."
        ],
        [
         "44",
         "Wait.",
         "Attendez."
        ],
        [
         "45",
         "Begin.",
         "Commencez."
        ],
        [
         "46",
         "Begin.",
         "Commence."
        ],
        [
         "47",
         "Go on.",
         "Poursuis."
        ],
        [
         "48",
         "Go on.",
         "Continuez."
        ],
        [
         "49",
         "Go on.",
         "Poursuivez."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 100
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Get up.</td>\n",
       "      <td>Lève-toi !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Get up.</td>\n",
       "      <td>Debout.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Go now.</td>\n",
       "      <td>Va, maintenant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Go now.</td>\n",
       "      <td>Allez-y maintenant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Go now.</td>\n",
       "      <td>Vas-y maintenant.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    english               french\n",
       "0       Go.                 Va !\n",
       "1       Go.              Marche.\n",
       "2       Go.           En route !\n",
       "3       Go.              Bouge !\n",
       "4       Hi.              Salut !\n",
       "..      ...                  ...\n",
       "95  Get up.           Lève-toi !\n",
       "96  Get up.              Debout.\n",
       "97  Go now.      Va, maintenant.\n",
       "98  Go now.  Allez-y maintenant.\n",
       "99  Go now.    Vas-y maintenant.\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mt.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 232736 entries, 0 to 232735\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   english  232736 non-null  object\n",
      " 1   french   232736 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_mt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning and preparation\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 2.1 </b>\n",
    "\n",
    "- To simplify the problem (smaller vocabulary), lower all capital letters.\n",
    "- Can we apply other data cleaning operations ? Explain your answer.\n",
    "- Split your data into training (80%) and test (20%) subsets using <code>train_test_split</code>, random seed = 42 and <code>shuffle</code> set to True.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "⚠️⚠️⚠️ Once done, note that vocabulary size and token index will be exclusively based on the train dataset. Make sure you choose the same random seed and other arguments specified in the question. ⚠️⚠️⚠️\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4449/354221495.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_mt_clean = df_mt.applymap(lambda s: s.lower() if type(s) == str else s)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# lower case\n",
    "df_mt_clean = df_mt.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "\n",
    "# Split data into train and test\n",
    "df_mt_train, df_mt_test = train_test_split(df_mt_clean, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "df_mt_train.reset_index(inplace=True, drop=True)\n",
    "df_mt_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "english",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "french",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "db8f042b-7727-4338-bea0-d4d0bd5adb62",
       "rows": [
        [
         "0",
         "turn the volume down.",
         "baisse le volume."
        ],
        [
         "1",
         "my sister is constantly reading comic books.",
         "ma sœur passe son temps à lire des bandes dessinées."
        ],
        [
         "2",
         "we have plans.",
         "nous avons des plans."
        ],
        [
         "3",
         "tom left without warning.",
         "thomas est parti sans crier gare."
        ],
        [
         "4",
         "we're not done.",
         "nous n'en avons pas terminé."
        ],
        [
         "5",
         "the concert will start at 2:30.",
         "le concert commencera à 14 h 30."
        ],
        [
         "6",
         "dinner's almost ready.",
         "le dîner est presque prêt."
        ],
        [
         "7",
         "the vault's already open.",
         "la salle des coffres est déjà ouverte."
        ],
        [
         "8",
         "i'm used to working hard.",
         "je suis habituée à travailler dur."
        ],
        [
         "9",
         "we're teachers.",
         "nous sommes enseignants."
        ],
        [
         "10",
         "i'll give you to the count of three.",
         "je te donne jusqu'à trois."
        ],
        [
         "11",
         "here it is.",
         "voilà."
        ],
        [
         "12",
         "the neighbors called the police.",
         "les voisins appelèrent la police."
        ],
        [
         "13",
         "i really need to talk to you privately.",
         "il me faut vraiment te parler en privé."
        ],
        [
         "14",
         "i can't go with you.",
         "je ne peux pas y aller avec toi."
        ],
        [
         "15",
         "relax and be yourself.",
         "détendez-vous et soyez vous-même."
        ],
        [
         "16",
         "you need to be prepared.",
         "il vous faut être préparée."
        ],
        [
         "17",
         "is this seat vacant?",
         "ce siège est-il libre ?"
        ],
        [
         "18",
         "i don't know what i'm doing here.",
         "je ne sais pas ce que je fais là."
        ],
        [
         "19",
         "i'm tired of him bawling me out.",
         "j'en ai marre qu'il m'engueule."
        ],
        [
         "20",
         "i thought i was being smart.",
         "je pensais être malin."
        ],
        [
         "21",
         "you're the boss.",
         "vous êtes le chef."
        ],
        [
         "22",
         "even adults do a lot of stupid things.",
         "même les adultes font beaucoup de bêtises."
        ],
        [
         "23",
         "i had a very good time.",
         "j'ai passé du très bon temps."
        ],
        [
         "24",
         "nuclear power is safe.",
         "l'énergie nucléaire est sans risque."
        ],
        [
         "25",
         "no system is perfect.",
         "aucun système n'est parfait."
        ],
        [
         "26",
         "tom and mary are looking for john.",
         "tom et mary cherchent john."
        ],
        [
         "27",
         "say it to his face, not behind his back.",
         "dites-le-lui en face, pas derrière son dos."
        ],
        [
         "28",
         "i'd like to discuss this with your boss.",
         "j'aimerais discuter de ça avec ton chef."
        ],
        [
         "29",
         "i've heard that name somewhere before.",
         "j'ai déjà entendu ce nom quelque part."
        ],
        [
         "30",
         "he died of natural causes.",
         "il est mort de mort naturelle."
        ],
        [
         "31",
         "don't change lanes without signaling.",
         "ne change pas de voie sans mettre ton clignotant."
        ],
        [
         "32",
         "i must leave on monday.",
         "je dois partir lundi."
        ],
        [
         "33",
         "come on!",
         "allons !"
        ],
        [
         "34",
         "i wore a white shirt.",
         "j'ai porté une chemise blanche."
        ],
        [
         "35",
         "nobody wants to be hated.",
         "personne ne souhaite être détesté."
        ],
        [
         "36",
         "it's all over for us.",
         "pour nous, tout est fini."
        ],
        [
         "37",
         "i paid him the money last week.",
         "je l’ai payé la semaine dernière."
        ],
        [
         "38",
         "tom is wary.",
         "tom se méfie."
        ],
        [
         "39",
         "you're very understanding.",
         "vous êtes fort compréhensif."
        ],
        [
         "40",
         "she takes after her mother.",
         "elle ressemble à sa mère."
        ],
        [
         "41",
         "i think they like you.",
         "je pense qu'ils t'apprécient."
        ],
        [
         "42",
         "you're bleeding.",
         "vous saignez."
        ],
        [
         "43",
         "those men are cowards.",
         "ces hommes sont des pleutres."
        ],
        [
         "44",
         "you're talkative.",
         "vous êtes bavards."
        ],
        [
         "45",
         "i don't think it'll rain.",
         "je ne pense pas qu'il va pleuvoir."
        ],
        [
         "46",
         "i have to button my jacket.",
         "il faut que je boutonne ma veste."
        ],
        [
         "47",
         "we have to escape.",
         "nous devons nous échapper."
        ],
        [
         "48",
         "he warned us not to enter the room.",
         "il nous avertit de ne pas pénétrer dans la pièce."
        ],
        [
         "49",
         "the apprentice is lazy.",
         "l'apprenti est paresseux."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 186188
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>turn the volume down.</td>\n",
       "      <td>baisse le volume.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my sister is constantly reading comic books.</td>\n",
       "      <td>ma sœur passe son temps à lire des bandes dess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we have plans.</td>\n",
       "      <td>nous avons des plans.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tom left without warning.</td>\n",
       "      <td>thomas est parti sans crier gare.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we're not done.</td>\n",
       "      <td>nous n'en avons pas terminé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186183</th>\n",
       "      <td>i doubt whether he is honest.</td>\n",
       "      <td>je doute qu'il soit honnête.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186184</th>\n",
       "      <td>i've missed out on so much.</td>\n",
       "      <td>j'ai tant manqué.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186185</th>\n",
       "      <td>that's what i'm talking about.</td>\n",
       "      <td>c'est de ça que je parle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186186</th>\n",
       "      <td>i've already read today's paper.</td>\n",
       "      <td>j'ai déjà lu le journal d'aujourd'hui.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186187</th>\n",
       "      <td>is tom still in the hospital?</td>\n",
       "      <td>tom est-il toujours à l'hôpital ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186188 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0                              turn the volume down.   \n",
       "1       my sister is constantly reading comic books.   \n",
       "2                                     we have plans.   \n",
       "3                          tom left without warning.   \n",
       "4                                    we're not done.   \n",
       "...                                              ...   \n",
       "186183                 i doubt whether he is honest.   \n",
       "186184                   i've missed out on so much.   \n",
       "186185                that's what i'm talking about.   \n",
       "186186              i've already read today's paper.   \n",
       "186187                 is tom still in the hospital?   \n",
       "\n",
       "                                                   french  \n",
       "0                                       baisse le volume.  \n",
       "1       ma sœur passe son temps à lire des bandes dess...  \n",
       "2                                   nous avons des plans.  \n",
       "3                       thomas est parti sans crier gare.  \n",
       "4                            nous n'en avons pas terminé.  \n",
       "...                                                   ...  \n",
       "186183                       je doute qu'il soit honnête.  \n",
       "186184                                  j'ai tant manqué.  \n",
       "186185                          c'est de ça que je parle.  \n",
       "186186             j'ai déjà lu le journal d'aujourd'hui.  \n",
       "186187                  tom est-il toujours à l'hôpital ?  \n",
       "\n",
       "[186188 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mt_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Machine Translation, we will rely on sequence-to-sequence (Seq2Seq) models. \n",
    "\n",
    "### Seq2Seq: Encoder/Decoder\n",
    "\n",
    "Seq2Seq has been first introduced by [Google](https://arxiv.org/abs/1409.3215). It captures the information carried by the input sequences in a low-dimensional encoded form and generates relevant output sequences in an iterative manner. Seq2Seq relies on two main blocks: one to read the input sequence (encoder) and another to generate the output sequence (decoder). \n",
    "\n",
    "**Encoder**  \n",
    "The encoder processes the input sequence. It reads the input data **one token at a time** and transform it into a **context vector** (fixed-sized vector), capturing the data's essential information. The encoder (RNN, LSTM, or GRU) traverses the input sequence, updating its internal state at each step. By the end of this process, the internal state of the encoder is a compact representation of the entire input sequence.\n",
    "\n",
    "**Decoder**  \n",
    "The decoder is tasked with generating the output sequence. Starting from the **context vector** produced by the encoder, it generates the output elements one at a time. Like the encoder, the decoder is often implemented as an RNN, LSTM, or GRU. It uses the context vector and what it has generated so far to predict the next element in the output sequence. This process is iterative and continues until a special **end-of-sequence token** is generated (or some other stopping criterion).\n",
    "\n",
    "At each decoding step, the output of the decoder is passed through a **dense layer followed by a softmax function**. The softmax produces a probability distribution over the entire **target vocabulary**, allowing the model to select the most likely next token. This softmax layer is crucial, as it transforms the decoder’s output into a meaningful prediction at every timestep.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"figures/seq2seqmodel.png\" width=\"80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Now we know the type of model we will use, let's move to tokenization. We will rely on character-to-character machine translation; therefore, sentences will be tokenized at <b>character-level</b>.\n",
    "\n",
    "Note however that there are two additional special tokens that we need for Seq2Seq models: the Start-of-Sequence (SOS) token and the End-of-Sequence (EOS). \n",
    "\n",
    "- The SOS is essential for initializing the decoding process. During training, it serves as the first input to the decoder, signaling the model to begin generating the target sequence. This helps the model learn a consistent starting context across all examples. During inference (i.e., when generating a translation), the SOS token is explicitly fed as the first decoder input, which triggers the model to begin producing the output sequence.\n",
    "\n",
    "- The End-of-Sequence (EOS) token, on the other hand, indicates when the generated sequence should stop. It is included in the target output during training so the model can learn to associate certain contexts with the end of a sentence. During inference, the model continues generating tokens until it predicts an EOS token, which signals that the output is complete. Without EOS, the model might continue generating irrelevant tokens or cut off the translation prematurely. Together, SOS and EOS provide clear boundaries for structured and meaningful sequence generation.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"figures/tokenization.png\" width=\"40%\"/>\n",
    "  <figcaption>Author: Shann Khosla<figcaption/>\n",
    "</div>\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 2.2 </b>\n",
    "\n",
    "- Add a Start-Of-Sentence (SOS) characters `\\t` and End-Of-Sentence (EOS) character `\\n` to <b>each french sentence (target)</b>. For example, \"bonjour!\" becomes \"\\tbonjour!\\n\"\n",
    "- What is the maximum sequence length for input text (English) and target text (French) ? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of characters per text for inputs (English): 290\n",
      "Max number of characters per text for outputs (French): 338\n"
     ]
    }
   ],
   "source": [
    "#Adding SOS and EOS tokens\n",
    "df_mt_train.french = df_mt_train.french.apply(lambda x: '\\t' + x + '\\n')\n",
    "df_mt_test.french = df_mt_test.french.apply(lambda x: '\\t' + x + '\\n')\n",
    "\n",
    "# Max seq lenght for english and french\n",
    "max_fr_seq_len = df_mt_train.french.str.len().max()\n",
    "max_en_seq_len = df_mt_train.english.str.len().max()\n",
    "\n",
    "print(f\"Max number of characters per text for inputs (English): {max_en_seq_len}\")\n",
    "print(f\"Max number of characters per text for outputs (French): {max_fr_seq_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "<b> Exercise 2.3 </b>\n",
    "- Build a vocabulary for the input text (English) and a vocabulary for the target text (French) using only <b>train dataset</b>. What is the size of each vocabulary ? \n",
    "- Build an input token-to-index dictionary to map each character (key) from the previously constructed English vocabulary to a unique index (value). \n",
    "- Build a target token-to-index dictionary to map each character (key) from the previously constructed French vocabulary to a unique index (value).\n",
    "Make sure that the character <code>\\t</code> is associated with index 0 and <code>\\n</code> is associated with index 1.     \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of English vocabulary: 62\n",
      "Size of French vocabulary: 79\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "vocab_en = []\n",
    "for text in df_mt_train.english:\n",
    "    for char in text:\n",
    "        if char not in vocab_en:\n",
    "            vocab_en.append(char)\n",
    "vocab_en.sort()\n",
    "vocab_fr = []\n",
    "for text in df_mt_train.french:\n",
    "    for char in text:\n",
    "        if char not in vocab_fr:\n",
    "            vocab_fr.append(char)\n",
    "vocab_fr.sort()\n",
    "print(\"Size of English vocabulary:\", len(vocab_en))\n",
    "print(\"Size of French vocabulary:\", len(vocab_fr))\n",
    "\n",
    "# Token to index lookup dicts for english and french\n",
    "en_token2index_dict = {token: idx for idx, token in enumerate(vocab_en)}\n",
    "fr_token2index_dict = {token: idx for idx, token in enumerate(vocab_fr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_token2index_dict['\\t']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training vs. Inference in Seq2Seq Models\n",
    "\n",
    "Although the architecture of a Seq2Seq model (encoder + decoder) stays the same, its **behavior during training and inference is very different** — and it’s important to understand why.\n",
    "\n",
    "##### **Training Phase** – *Using the Right Answer*\n",
    "\n",
    "During training, we use a method called **teacher forcing**. This means that at each time step, the decoder is given the **correct previous token** from the target sentence (french) — not the one the model predicted.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"figures/teacher_forcing.png\" width=\"40%\"/>\n",
    "  <figcaption>Author: Wanshun Wong<figcaption/>\n",
    "</div>\n",
    "\n",
    "At each step, the model learns to predict the **next token** given the **true previous one**. This makes training faster and more stable because the decoder always knows the \"right context,\" even if it would have made a mistake on its own.\n",
    "\n",
    "**The model is trained to minimize the difference between its predicted output sequence and the true output sequence, using a loss function like cross-entropy.**\n",
    "\n",
    "##### **Inference Phase** – *Using Its Own Predictions*\n",
    "\n",
    "When we switch to inference (i.e., generating a translation), the model doesn’t have access to the ground-truth translation anymore. It has to **generate the output one token at a time**, feeding **its own previous prediction** back into the decoder.\n",
    "\n",
    "It starts with the **start-of-sequence token (SOS)** and continues predicting the next token based on everything it has generated so far. It stops when it outputs the **end-of-sequence token (EOS)** or hits a maximum sequence length.\n",
    "\n",
    "This process is called **autoregressive decoding**, and it's harder because:\n",
    "- There is no ground truth to guide the model\n",
    "- One small error early on can affect all the next predictions\n",
    "\n",
    "##### Summary\n",
    "\n",
    "| Phase     | What is fed to the decoder?          | Purpose                            |\n",
    "|-----------|--------------------------------------|------------------------------------|\n",
    "| Training  | The correct previous token (teacher forcing) | To help the model learn faster and more accurately |\n",
    "| Inference | The model’s own previous prediction  | To test whether the model can generate coherent sequences by itself |\n",
    "\n",
    "Understanding this distinction is key: **teacher forcing helps the model learn**, while **inference checks whether it really has**.\n",
    "\n",
    "\n",
    "---- \n",
    "\n",
    "Knowing this, let's proceed to the vectorization using one-hot encoding. \n",
    "\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 2.4 </b>\n",
    "\n",
    "- Write a function `one_hot_input` that prepares:\n",
    "    - the encoder's inputs `encoder_one_hot_inputs` as a 3D array of shape `(size_corpus, max_english_sentence_length, size_english_vocabulary)` containing the one-hot vectorization of the English sentences. The sentences shorter than `max_english_sentence_length` are to be filled with spaces `\" \"` (padding). Replace out-of-vocabulary tokens by a space `\" \"`.\n",
    "    - the decoder's inputs `decoder_one_hot_inputs` as a 3D array of shape `(size_corpus, max_french_sentence_length, size_french_vocabulary)` containing the one-hot vectorization of the French sentences. The sentences shorter than `max_french_sentence_length` are to be filled with spaces `\" \"`. Replace out-of-vocabulary tokens by a space `\" \"`. \n",
    "</div>\n",
    "\n",
    "Consistently with the teacher forcing approach, notice how the function `one_hot_target` that prepares the decoder's target `decoder_one_hot_targets` is the same as `decoder_one_hot_inputs` but offset by one step: `decoder_one_hot_targets[:, j, :]` $\\leftarrow$ `decoder_one_hot_inputs[:, j + 1, :]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186188"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_mt_train.english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_input(corpus, token2index_dict, max_seq_len):\n",
    "    result = np.zeros((len(corpus), max_seq_len, len(token2index_dict)), dtype=\"bool\")\n",
    "    for i, document in enumerate(corpus):\n",
    "        for t, char in enumerate(document):\n",
    "            try:\n",
    "                result[i, t, token2index_dict[char]] = 1\n",
    "                #padding with space if length exceeded\n",
    "            except:\n",
    "                result[i, t, token2index_dict[\" \"]] = 1\n",
    "                continue\n",
    "        result[i, t + 1 :, token2index_dict[\" \"]] = 1.0\n",
    "    return result\n",
    "\n",
    "\n",
    "def one_hot_target(corpus, token2index_dict, max_seq_len):\n",
    "    result = np.zeros(\n",
    "        (len(corpus), max_seq_len, len(token2index_dict)), dtype=\"bool\"\n",
    "    )\n",
    "    for i, document in enumerate(corpus):\n",
    "        for t, char in enumerate(document):\n",
    "            if t > 0:\n",
    "                try:\n",
    "                    result[i, t - 1, token2index_dict[char]] = 1\n",
    "                except:\n",
    "                    result[i, t - 1, token2index_dict[\" \"]] = 1\n",
    "                    continue\n",
    "        result[i, t:, token2index_dict[\" \"]] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "encoder_one_hot_inputs = one_hot_input(\n",
    "    df_mt_train.english, en_token2index_dict, max_en_seq_len\n",
    ")\n",
    "decoder_one_hot_inputs = one_hot_input(\n",
    "    df_mt_train.french, fr_token2index_dict, max_fr_seq_len\n",
    ")\n",
    "decoder_one_hot_targets = one_hot_target(\n",
    "    df_mt_train.french, fr_token2index_dict, max_fr_seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Training \n",
    "\n",
    "\n",
    "We will now implement the architecture shown in the figure below to train a Seq2Seq model for character-level machine translation.\n",
    "\n",
    "On the left, the encoder processes a sequence of one-hot encoded input characters (from an English sentence) using a stack of GRU cells. As the encoder reads each character, it updates its internal hidden state. Once the final character is processed, the last hidden state becomes the context vector summarizing the entire input sentence.\n",
    "\n",
    "This context vector is then passed to the decoder on the right. The decoder is also composed of GRU cells, and at each time step, it receives:\n",
    "\n",
    "* The one-hot encoded representation of the previous character (from the target sentence during training, or from its own prediction during inference)\n",
    "\n",
    "* The current hidden state (initially, the final hidden state from the encoder)\n",
    "\n",
    "The decoder's GRU outputs are passed through a Dense layer followed by a softmax activation, producing a probability distribution over the target vocabulary at each time step. This allows the model to predict the next character in the output sequence. The decoder continues this autoregressive process until it generates a special end-of-sequence (EOS) token.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"figures/seq2seqmodel.png\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 3.1 </b>\n",
    "\n",
    "- Briefly explain how a GRU layer works: What are its inputs, outputs, and why is it useful in the Seq2Seq context? \n",
    "- Examine and complement the implementation of the `seq2seq_model` function to build the Seq2Seq model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def seq2seq_model(latent_dim, input_tokenidx_dict, target_tokenidx_dict):\n",
    "    # Define an input sequence and process it.\n",
    "    x_encoder = tf.keras.Input(shape=(None, len(...)))\n",
    "    encoder = tf.keras.layers.GRU(latent_dim, return_state=True)\n",
    "    y_encoder, h_encoder = encoder(x_encoder)\n",
    "\n",
    "    # Set up the decoder input sequence\n",
    "    x_decoder = tf.keras.Input(shape=(None, len(...)))\n",
    "\n",
    "    # We set up the decoder to return output sequences and hidden states\n",
    "    # We don't use the return states in the training model, but we will use them in inference.\n",
    "    decoder = tf.keras.layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "    # use `h_encoder` as initial state.\n",
    "    y_decoder, _ = decoder(x_decoder, initial_state=...)\n",
    "    # Output layer, Dense + softmax activation on\n",
    "    dense_softmax = tf.keras.layers.Dense(len(...), activation=\"softmax\")\n",
    "    y_decoder = dense_softmax(y_decoder)\n",
    "\n",
    "    # Define the model that will turn [`x_encoder`, `x_decoder`] into `y_decoder`\n",
    "    model = tf.keras.Model([x_encoder, x_decoder], y_decoder)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b> Exercise 3.2 </b>\n",
    "\n",
    "- Instantiate and train the Seq2Seq model using the teacher forcing approach.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 512  # dim of the latent space.\n",
    "\n",
    "# Instantiate the model and compile it\n",
    "model = seq2seq_model(..., ..., ...)\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Set early stopping if validation accuracy does not improve after `patience` epochs\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\", patience=50, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    [..., ...],  # inputs\n",
    "    ...,  # targets\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Inference \n",
    "\n",
    "Unlike training, the target sequence is not known during inference. The model generates the output sequence character by character, starting from an SOS character until it generates an EOS character, signifying the end of the output sequence. Since the true output tokens are not available, the model uses its own predictions as input for the next step. This process is autoregressive and continues until the model produces the EOS or reaches a maximum length. \n",
    "\n",
    "During inference, the model generates the target sequence character by character, in an autoregressive way. \n",
    "\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 4.1 </b>\n",
    "\n",
    "- Examine the implementation of the `translate` function and explain how the Seq2Seq model is used for inference.\n",
    "- Run the inference on some test sentences one by one.\n",
    "- What do you notice about errors with this autoregressive generation? Are mistakes equally distributed throughout a sequence (begining, middle and end)?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_seq, seq2seq_model=model):\n",
    "    # You can check seq2seq_model.summary() to recall the input and outputs\n",
    "    # of its layers\n",
    "\n",
    "    # Encoder\n",
    "    x_encoder = seq2seq_model.input[0]  # input_1\n",
    "    y_encoder, h_encoder = seq2seq_model.layers[2].output  # lstm_1\n",
    "    encoder = tf.keras.Model(inputs=x_encoder, outputs=h_encoder)\n",
    "\n",
    "    # Decoder\n",
    "    x_decoder = seq2seq_model.input[1]  # input_2\n",
    "    h_in_decoder = tf.keras.Input(shape=(latent_dim,))\n",
    "    gru_layer = seq2seq_model.layers[3]\n",
    "    y_gru, h_out_gru = gru_layer(x_decoder, initial_state=h_in_decoder)\n",
    "    dense_layer = seq2seq_model.layers[4]\n",
    "    y_decoder = dense_layer(y_gru)\n",
    "    decoder_model = tf.keras.Model([x_decoder, h_in_decoder], [y_decoder, h_out_gru])\n",
    "\n",
    "    # Reverse dictionary to recover target vocabulary from token index\n",
    "    token_idx_fr_dict = dict((i, char) for char, i in fr_token2index_dict.items())\n",
    "\n",
    "    def decode_translation(input_seq):\n",
    "        # Encode input as context vector.\n",
    "        h_encoder = encoder.predict(input_seq, verbose=0)\n",
    "\n",
    "        # Generate empty target sequence of length 1.\n",
    "        translated_seq = np.zeros((1, 1, len(fr_token2index_dict)))\n",
    "        # Initialize the first character of target sequence with the SOS.\n",
    "        translated_seq[0, 0, fr_token2index_dict[\"\\t\"]] = 1.0\n",
    "\n",
    "        # Sampling loop for a batch (=1) of sequences\n",
    "        condition = False\n",
    "        decoded_sentence = \"\"\n",
    "        h_in_decoder = h_encoder  # init\n",
    "        while not condition:\n",
    "            y_decoder, h_out_decoder = decoder_model.predict(\n",
    "                [translated_seq, h_in_decoder], verbose=0\n",
    "            )\n",
    "\n",
    "            # Get token\n",
    "            token_index = np.argmax(y_decoder[0, -1, :])\n",
    "            char = token_idx_fr_dict[token_index]\n",
    "            decoded_sentence += char\n",
    "\n",
    "            # Stop condition: find stop character or reach max length\n",
    "            if char == \"\\n\" or len(decoded_sentence) > max_fr_seq_len:\n",
    "                condition = True\n",
    "\n",
    "            # Update the translated sequence.\n",
    "            translated_seq = np.zeros((1, 1, len(fr_token2index_dict)))\n",
    "            translated_seq[0, 0, token_index] = 1.0\n",
    "\n",
    "            # Update hidden state\n",
    "            h_in_decoder = h_out_decoder\n",
    "        return decoded_sentence\n",
    "\n",
    "    return decode_translation(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test input sequences\n",
    "input_sequences = ...\n",
    "\n",
    "\n",
    "for i, seq_index in enumerate(range(20)):\n",
    "\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = input_sequences[seq_index : seq_index + 1]\n",
    "    # encoder_one_hot_inputs[seq_index : seq_index + 1]\n",
    "    translated_sentence = ...\n",
    "\n",
    "    print(f\"-------------------- Sentence {i} --------------------\")\n",
    "    print(\"Input sentence:\", df_mt_test.english[seq_index])\n",
    "    print(\"Translated sentence:\", translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU score is one of the most common metrics used to evaluate the quality of machine translation models. It measures how closely the model’s output matches one or more reference translations.\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 4.2 </b>\n",
    "\n",
    "* Briefly explain how the BLEU score works. What does it measure, and how is it computed?\n",
    "\n",
    "* Using `nltk.translate.bleu_score`, compute the average BLEU score over at least 100 sentence pairs (predicted vs. reference). Interpret the result: what does it say about the model's performance?\n",
    "\n",
    "* Suggest some ways to improve the performance. For example, you can consider data quality, preparation, vectorization or model architecture.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
